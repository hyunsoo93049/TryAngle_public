# TryAngle v1.5 ì™„ì „ ì„¤ê³„ ë¬¸ì„œ

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

### í•µì‹¬ ì»¨ì…‰
êµ¬ë„ì™€ í”„ë ˆì´ë°ì— ì§‘ì¤‘í•œ AI ì¹´ë©”ë¼ ê°€ì´ë“œ ì•±
- **í¬ì¦ˆ ì¤‘ì‹¬ (v1)** â†’ **êµ¬ë„/í”„ë ˆì´ë° ì¤‘ì‹¬ (v1.5)**
- 2,132ì¥+ ë ˆí¼ëŸ°ìŠ¤ ì‚¬ì§„ì˜ "ì¢‹ì€ êµ¬ë„" íŒ¨í„´ í•™ìŠµ
- ì‹¤ì‹œê°„ ì •ëŸ‰ì  í”¼ë“œë°± ì œê³µ

### í•µì‹¬ í˜ì‹ : ì••ì¶•ê° ê¸°ë°˜ ê±°ë¦¬ í”¼ë“œë°±
ê¸°ì¡´ ì•±ë“¤ê³¼ì˜ ì°¨ë³„ì :
- âŒ ê¸°ì¡´: "ì–¼êµ´ ì¤‘ì•™ì—", "ê·¸ë¦¬ë“œ ë¼ì¸"
- âœ… v1.5: **"1.2m ë’¤ë¡œ ë˜ëŠ” ì¤Œ ì¸"** (ê±°ë¦¬ + ì••ì¶•ê°)

---

## ğŸ—ï¸ ì „ì²´ ì•„í‚¤í…ì²˜

```
ì˜¤í”„ë¼ì¸ í•™ìŠµ (RTX 4070 Super - í•œ ë²ˆë§Œ)
    â†“
ë ˆí¼ëŸ°ìŠ¤ 2,132ì¥+ ì •ë°€ ë¶„ì„
    â†“
í…Œë§ˆë³„ íŒ¨í„´ DB (JSON 2-5MB)
    â†“
ì•±ì— ë‚´ì¥
    â†“
ì‹¤ì‹œê°„ ì´¬ì˜ (iPhone 15fps)
```

---

## ğŸš€ V1.5ë¡œ ì–»ì„ ìˆ˜ ìˆëŠ” ì‹¤ì§ˆì  ì´ì 

### ğŸ“¸ ì‚¬ìš©ì ê²½í—˜ ì¸¡ë©´

#### 1. **ì •ëŸ‰ì ì´ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±**
- ê¸°ì¡´: "ì¡°ê¸ˆ ë” ë’¤ë¡œ" â†’ V1.5: "1.2m ë’¤ë¡œ ê°€ê±°ë‚˜ 35mmë¡œ ì¡°ì •"
- ê¸°ì¡´: "êµ¬ë„ ì¡°ì •" â†’ V1.5: "ì¸ë¬¼ì„ ì™¼ìª½ 30cmë¡œ, ì¹´í˜ ì°½ë¬¸ê³¼ ê· í˜• ë§ì¶¤"
- ê¸°ì¡´: "ê°ë„ ë³€ê²½" â†’ V1.5: "ì¹´ë©”ë¼ 12ë„ ìœ„ë¡œ, í…Œì´ë¸” ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨"

#### 2. **ìƒí™©ë³„ ë§ì¶¤ ê°€ì´ë“œ**
ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ Theme Ã— Pose Type ì¸ì‹:
- "ì¹´í˜ ë°˜ì‹  ìƒ·" â†’ 2,000ì¥ì˜ ì„±ê³µ íŒ¨í„´ê³¼ ë¹„êµ
- "ê³µì› ì „ì‹  ìƒ·" â†’ ì•¼ì™¸ ì „ì‹  ìµœì  íŒ¨í„´ ì ìš©
- í…Œë§ˆë³„ íŠ¹í™” íŒ (ì¹´í˜ëŠ” ì°½ë¬¸ í™œìš©, ê³µì›ì€ ë‚˜ë¬´ í™œìš©)

#### 3. **ì „ë¬¸ê°€ ë…¸í•˜ìš° ë‚´ì¬í™”**
- ì¸ìŠ¤íƒ€ê·¸ë¨ ì¸í”Œë£¨ì–¸ì„œë“¤ì˜ êµ¬ë„ ë¹„ë²•
- ì „ë¬¸ ì‚¬ì§„ì‘ê°€ë“¤ì˜ ì••ì¶•ê° í™œìš©ë²•
- ì¥ì†Œë³„ ìµœì  ì•µê¸€ (ì°½ê°€ì„, ì½”ë„ˆì„ ë“±)

### ğŸ’° ë¹„ì¦ˆë‹ˆìŠ¤ ì¸¡ë©´

- **ëª…í™•í•œ ì°¨ë³„ì **: ì„¸ê³„ ìµœì´ˆ "ì••ì¶•ê° ê¸°ë°˜ ê±°ë¦¬ í”¼ë“œë°±"
- **íŠ¹í—ˆ ê°€ëŠ¥ì„±**: ê¸°ìˆ ì  í˜ì‹ ìœ¼ë¡œ ë³´í˜¸ ê°€ëŠ¥
- **í™•ì¥ì„±**: ìƒˆ íŠ¸ë Œë“œ ì¦‰ì‹œ ë°˜ì˜ (ì´ë¯¸ì§€ ì¶”ê°€ë§Œìœ¼ë¡œ)

---

## ğŸ”¬ ê°œì„ ëœ íŒŒì´í”„ë¼ì¸ (v1.2)

### ì™„ì „ ìë™í™” One-Pass Processing

ê¸°ì¡´ì˜ 2-pass (ë¶„ë¥˜ â†’ íŠ¹ì§•ì¶”ì¶œ) ëŒ€ì‹  1-passë¡œ íš¨ìœ¨í™”:

```python
class ImprovedV15Pipeline:
    def __init__(self):
        # ëª¨ë“  ëª¨ë¸ í•œ ë²ˆë§Œ ë¡œë“œ
        self.models = {
            'grounding_dino': load_grounding_dino(),
            'rtmpose': load_rtmpose(),
            'depth': load_depth_anything(),
        }

    def process_batch(self, images):
        # 1. ëª¨ë“  íŠ¹ì§• ë™ì‹œ ì¶”ì¶œ (GPU ë³‘ë ¬)
        features = {
            'objects': self.models['grounding_dino'](images),
            'poses': self.models['rtmpose'](images),
            'depths': self.models['depth'](images),
        }

        # 2. íŠ¹ì§• ê¸°ë°˜ ìë™ ë¶„ë¥˜
        for i, img in enumerate(images):
            theme = classify_theme(features['objects'][i])
            pose_type = classify_pose(features['poses'][i])

            # 3. ë¶„ë¥˜ì™€ íŠ¹ì§• ë™ì‹œ ì €ì¥
            save_with_metadata(img, theme, pose_type, features[i])

        return features
```

### ë‹¤ì¤‘ ì‹ í˜¸ ê¸°ë°˜ Robust Classification

ë‹¨ì¼ ëª¨ë¸ ì˜ì¡´ ëŒ€ì‹  ì—¬ëŸ¬ ì‹ í˜¸ë¥¼ ì¢…í•©:

```python
def robust_pose_classification(image, features):
    # 1. RTMPose ê¸°ë³¸ íŒì •
    pose_rtm = rtmpose_classify(features['pose'])

    # 2. Bbox ë¹„ìœ¨ë¡œ ë³´ì •
    bbox_ratio = features['bbox'].height / image.height
    if bbox_ratio > 0.8:
        pose_bbox = "full_body"
    elif bbox_ratio > 0.5:
        pose_bbox = "half_body"

    # 3. Depth ì •ë³´ í™œìš©
    visible_range = analyze_depth_range(features['depth'])
    pose_depth = depth_to_pose_type(visible_range)

    # 4. Votingìœ¼ë¡œ ìµœì¢… ê²°ì •
    return majority_vote([pose_rtm, pose_bbox, pose_depth])
```

### í…Œë§ˆ ë¶„ë¥˜ ê·œì¹™ (ìë™í™”)

```python
THEME_RULES = {
    'cafe_indoor': ['table', 'chair', 'coffee', 'window', 'cup'],
    'street_urban': ['building', 'car', 'road', 'sidewalk', 'sign'],
    'park_nature': ['tree', 'grass', 'bench', 'sky', 'flower'],
    'beach': ['ocean', 'sand', 'wave', 'umbrella'],
    'winter': ['snow', 'ice', 'coat', 'scarf']
}

def auto_classify_theme(detected_objects):
    scores = {}
    for theme, keywords in THEME_RULES.items():
        score = sum(1 for obj in detected_objects if obj in keywords)
        scores[theme] = score
    return max(scores, key=scores.get)
```

---

## ğŸ”¬ Phase 1: ì˜¤í”„ë¼ì¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸

### ì‚¬ìš© í™˜ê²½
- GPU: RTX 4070 Super
- ì´ë¯¸ì§€: 2,132ì¥ (MVP 300ì¥ë„ ê°€ëŠ¥)
- ì´ ì†Œìš” ì‹œê°„: 2-3ì‹œê°„ (2,132ì¥ ê¸°ì¤€)
- ê²°ê³¼ë¬¼: 2-5MB JSON íŒŒì¼

### Step 1: í…Œë§ˆ ìë™ ë¶„ë¥˜

**ëª©ì :** 2,132ì¥ì„ í…Œë§ˆë³„ë¡œ ê·¸ë£¹í™”

**ë°©ë²• 1 (ê¶Œì¥): Grounding DINO Rules-Based + DINOv2 Clustering**

```python
# Step 1-1: Grounding DINOë¡œ ë°°ê²½ ê°ì²´ ê²€ì¶œ
for image in 2132_photos:
    objects = grounding_dino(image,
        ["window", "beach", "tree", "street", "table", "snow"])

    # ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜
    if "window" in objects and "table" in objects:
        theme = "cafe_indoor"
    elif "beach" in objects:
        theme = "beach"
    elif "tree" in objects and not "street":
        theme = "park_nature"
    elif "snow" in objects:
        theme = "winter_outdoor"
    # ...

# Step 1-2: DINOv2ë¡œ ì• ë§¤í•œ ì¼€ì´ìŠ¤ clustering
ambiguous_images = [img for img in images if theme == "unknown"]
embeddings = dinov2_large(ambiguous_images)  # 384D vectors
clusters = kmeans(embeddings, n_clusters=5)
# ê° í´ëŸ¬ìŠ¤í„° ìˆ˜ë™ ë¼ë²¨ë§

# Step 1-3: ìˆ˜ë™ ê²€ì¦ (ì„ íƒì )
# ìƒ˜í”Œë§ìœ¼ë¡œ ì •í™•ë„ í™•ì¸ í›„ í•„ìš”ì‹œ ìˆ˜ì •
```

**ì •í™•ë„:**
- Grounding DINO Rules: 90-95%
- DINOv2 Clustering: 95-98% (ìˆ˜ë™ ë¼ë²¨ë§ í›„)

**ê²°ê³¼ ì˜ˆì‹œ:**
- cafe_indoor: 668ì¥
- park_nature: 908ì¥
- street_urban: 330ì¥
- winter_outdoor: 98ì¥
- highangle: 128ì¥

**ì†Œìš” ì‹œê°„:** ì•½ 30-40ë¶„ (2,132ì¥ ê¸°ì¤€)

---

### Step 2: ì •ë°€ ê°ì²´ ê²€ì¶œ

**ëª©ì :** ì¸ë¬¼ + ë°°ê²½ ê°ì²´ ì •í™•í•œ ìœ„ì¹˜ ì¶”ì¶œ

**ëª¨ë¸:** Grounding DINO-B (Base)

**ë°©ë²•:**
```python
for image in each_theme_group:
    # ì¸ë¬¼ ê²€ì¶œ
    person = grounding_dino(image, "person")

    # í…Œë§ˆë³„ ë°°ê²½ ê°ì²´
    if theme == "cafe":
        window = grounding_dino(image, "window")
        chair = grounding_dino(image, "chair")
        table = grounding_dino(image, "table")

    # ê³µê°„ ê´€ê³„ ê³„ì‚°
    person_to_window = calculate_distance(person, window)
    window_direction = get_direction(person, window)
```

**ì¶”ì¶œ ì •ë³´:**
```json
{
  "person_bbox": [250, 100, 300, 600],
  "background_objects": {
    "window": {
      "bbox": [720, 300, 300, 600],
      "direction_from_person": "right",
      "distance": 0.25
    }
  }
}
```

**ì†Œìš” ì‹œê°„:** ì•½ 1ì‹œê°„ (2,132ì¥ Ã— 1.5ì´ˆ)

---

### Step 2-1: Pose Type ìë™ ê²€ì¶œ (í•µì‹¬!)

**ëª©ì :** Shot Type ë¶„ë¥˜ (Samsung Display ê¸°ì¤€ ì°¸ê³ )

**ëª¨ë¸:** RTMPose (133 keypoints) via rtmlib (ONNX)

**ìƒˆë¡œìš´ 4-Type ë¶„ë¥˜ ì²´ê³„:**
| Shot Type | ì •ì˜ | íŒë‹¨ ê¸°ì¤€ |
|-----------|------|-----------|
| **closeup** | ì–¼êµ´~ì–´ê¹¨ | ì–´ê¹¨ë§Œ ë³´ì´ê±°ë‚˜ í”„ë ˆì„ í•˜ë‹¨ 70% ì´í•˜ |
| **medium_shot** | ê°€ìŠ´~í—ˆë¦¬ | íŒ”ê¿ˆì¹˜/ê³¨ë°˜ ë³´ì„ (bust+waist í†µí•©) |
| **knee_shot** | í—ˆë²…ì§€~ë¬´ë¦ | ë¬´ë¦ ë³´ì„ (í”„ë ˆì„ í•˜ë‹¨ 75% ì´í•˜) |
| **full_shot** | ì „ì‹  | ë°œëª© ë³´ì„ (í”„ë ˆì„ í•˜ë‹¨ 85% ì´í•˜) |

**ë°©ë²•:**
```python
for image in 2133_photos:
    # RTMPoseë¡œ keypoints ì¶”ì¶œ (rtmlib ONNX)
    keypoints, scores = wholebody(image)  # 133ê°œ (ì–¼êµ´+ëª¸+ì†)

    # 1. ì•‰ì€ ìì„¸ ê°ì§€ (í•µì‹¬!)
    def detect_sitting():
        torso_length = hip_y - shoulder_y
        hip_to_knee = knee_y - hip_y
        ratio = hip_to_knee / torso_length
        # ì„œ ìˆì„ ë•Œ: ratio â‰ˆ 1.0~1.5
        # ì•‰ì•„ ìˆì„ ë•Œ: ratio â‰ˆ 0.2~0.5
        return ratio < 0.6

    is_sitting = detect_sitting()

    # 2. Shot Type íŒì • (ì•‰ì€ ìì„¸ ê³ ë ¤)
    if ankle_visible and ankle_y > img_height * 0.85:
        pose_type = "full_shot"
    elif ankle_visible and is_sitting:
        pose_type = "knee_shot"  # ì•‰ì•„ìˆëŠ”ë° ë°œëª© ë³´ì„ â†’ knee_shot
    elif knee_visible and knee_y > img_height * 0.75:
        pose_type = "knee_shot"
    elif knee_visible and is_sitting:
        pose_type = "medium_shot"  # ì•‰ì•„ìˆëŠ”ë° ë¬´ë¦ ë³´ì„ â†’ medium_shot
    elif hip_visible or elbow_visible:
        pose_type = "medium_shot"
    elif shoulder_visible:
        pose_type = "closeup" if shoulder_y < img_height * 0.7 else "medium_shot"
    elif head_visible:
        pose_type = "closeup"
    else:
        pose_type = "unknown"
```

**Occlusion Handling (ì˜·/ê°€ë¦¼ ëŒ€ì‘):**
```python
# 1. Confidence threshold ë‚®ê²Œ ì„¤ì • (0.3)
# 2. ë³´ì´ëŠ” ê´€ì ˆë§Œìœ¼ë¡œ íŒë‹¨
# 3. Multi-frame averaging (ì‹¤ì‹œê°„ìš©)
# 4. Conservative classification
if uncertain:
    pose_type = "upper_body"  # ì•ˆì „í•˜ê²Œ ìƒë°˜ì‹ ìœ¼ë¡œ
```

**ê²°ê³¼ ì˜ˆì‹œ:**
```json
{
  "filename": "IMG_1234.jpg",
  "theme": "cafe_indoor",
  "pose_type": "medium_shot",
  "is_sitting": true,
  "sit_ratio": 0.35,
  "visible_keypoints": {
    "head": true,
    "shoulder": true,
    "elbow": true,
    "hip": true,
    "knee": true,
    "ankle": false
  }
}
```

**ì•‰ì€ ìì„¸ ê°ì§€ ì›ë¦¬:**
```
ì„œ ìˆì„ ë•Œ:           ì•‰ì•„ ìˆì„ ë•Œ:
   ğŸ‘¤ head               ğŸ‘¤ head
   â— shoulder            â— shoulder
   â— hip                 â— hip â†â”€â”€ ë¬´ë¦ê³¼ Y ê±°ë¦¬ ì§§ìŒ
   â— knee (ì•„ë˜)         â— knee (ê±°ì˜ ê°™ì€ ë†’ì´)
   â— ankle               â— ankle

ratio = (knee_y - hip_y) / (hip_y - shoulder_y)
ì„œ ìˆìŒ: 1.0~1.5 | ì•‰ì•„ ìˆìŒ: 0.2~0.5
```

**ì¤‘ìš”:** RTMPoseëŠ” 133ê°œ keypointë¥¼ ì œê³µí•˜ë¯€ë¡œ ì–¼êµ´ ëœë“œë§ˆí¬ê¹Œì§€ í™œìš© ê°€ëŠ¥!

**ì†Œìš” ì‹œê°„:** ì•½ 15ë¶„ (2,133ì¥ Ã— 0.5ì´ˆ, rtmlib ONNX ê¸°ì¤€)

---

### Step 3: ê¹Šì´ ë¶„ì„ (í•µì‹¬!)

**ëª©ì :** ì¹´ë©”ë¼ ê±°ë¦¬/ì••ì¶•ê° íŒë‹¨

**ëª¨ë¸:** Depth Anything V2 Large

**ë°©ë²•:**
```python
for image in 2132_photos:
    # Depth map ìƒì„±
    depth_map = depth_anything_v2_large(image)

    # í•µì‹¬ ì§€í‘œ ê³„ì‚°
    person_depth = mean(depth_map[person_mask])
    background_depth = mean(depth_map[0:h/3, :])
    foreground_depth = mean(depth_map[3*h/4:h, :])

    # Compression Index
    total_range = background_depth - foreground_depth
    compression_index = 1.0 - (total_range / 255.0)
    # 0 = ê´‘ê° (í° ë²”ìœ„)
    # 1 = ë§ì› (ì‘ì€ ë²”ìœ„, ì••ì¶•ë¨)
```

**ì£¼ì˜:** ì¹´ë©”ë¼ ê°ë„ëŠ” depth mapì´ ì•„ë‹Œ **iPhone Gyroscope**ë¡œ ê°ì§€!

**ì¶”ì¶œ ì •ë³´:**
```json
{
  "depth_analysis": {
    "person_depth": 165,
    "background_depth": 220,
    "foreground_depth": 80,
    "compression_index": 0.45,
    "camera_type": "normal_to_tele"
  }
}
```

**ì¹´ë©”ë¼ ê°ë„ ê°ì§€ (ì‹¤ì‹œê°„ë§Œ í•´ë‹¹):**
```swift
// iOS CMMotionManager ì‚¬ìš©
let motionManager = CMMotionManager()
if motionManager.isDeviceMotionAvailable {
    motionManager.startDeviceMotionUpdates(to: queue) { motion, error in
        let pitch = motion.attitude.pitch * 180 / .pi
        // pitch: -90 (í•˜í–¥) ~ 0 (ìˆ˜í‰) ~ 90 (ìƒí–¥)
        // ì¹´ë©”ë¼ ê°ë„: -pitch (ì¹´ë©”ë¼ëŠ” ë°˜ëŒ€ ë°©í–¥)
        let cameraAngle = -pitch
    }
}
// ì •í™•ë„: Â±2ë„ (95% ì‹ ë¢°ë„)
```

**ì†Œìš” ì‹œê°„:** ì•½ 40ë¶„

---

### Step 4: êµ¬ë„ íŠ¹ì§• ê³„ì‚°

**ê° ì‚¬ì§„ë§ˆë‹¤ ê³„ì‚°:**

1. **ìœ„ì¹˜ íŠ¹ì§•**
```python
person_center_x = (bbox.x + bbox.width/2) / image_width
person_center_y = (bbox.y + bbox.height/2) / image_height
â†’ (0.35, 0.42)
```

2. **í¬ê¸° íŠ¹ì§•**
```python
person_size_ratio = (bbox.width * bbox.height) / (img_w * img_h)
â†’ 0.32
```

3. **ì—¬ë°± íŠ¹ì§•**
```python
left_margin = bbox.x / image_width
right_margin = (image_width - bbox.x - bbox.width) / image_width
top_margin = bbox.y / image_height
bottom_margin = (image_height - bbox.y - bbox.height) / image_height
â†’ (0.22, 0.46, 0.18, 0.38)
```

4. **ì‚¼ë¶„í•  ì ìˆ˜**
```python
third_points = [(1/3, 1/3), (2/3, 1/3), (1/3, 2/3), (2/3, 2/3)]
min_distance = min(euclidean(person_center, point) for point in third_points)
rule_of_thirds_score = 1.0 - (min_distance / 0.5)
â†’ 0.92
```

**ì†Œìš” ì‹œê°„:** ì¦‰ì‹œ (ë‹¨ìˆœ ìˆ˜í•™)

---

### Step 5: Aspect Ratio ì¸ì‹ ë° ì²˜ë¦¬ (ì¤‘ìš”!)

**ëª©ì :** ì´¬ì˜ ì¢…íš¡ë¹„ì— ë”°ë¥¸ ìµœì  êµ¬ë„ ì°¨ë³„í™”

**ì™œ ì¤‘ìš”í•œê°€?**
```
1:1 (ì¸ìŠ¤íƒ€ ì •ì‚¬ê°í˜•)
â†’ ìœ„ì•„ë˜ ì—¬ë°± ë™ì¼, ì¤‘ì•™ ì§‘ì¤‘

4:3 (ê¸°ë³¸ ì¹´ë©”ë¼)
â†’ ì—¬ë°± ì—¬ìœ  ìˆìŒ

9:16 (ì„¸ë¡œ ì „ì²´ í™”ë©´)
â†’ ìœ„ì•„ë˜ ì—¬ë°± ë„‰ë„‰, ë°°ê²½ í™œìš©

16:9 (ê°€ë¡œ ì‹œë„¤ë§ˆí‹±)
â†’ ì¢Œìš° ì—¬ë°± ë§ìŒ, ë°°ê²½ ê°•ì¡°

â†’ ê°™ì€ í…Œë§ˆ+í¬ì¦ˆë¼ë„ ì¢…íš¡ë¹„ë§ˆë‹¤ ìµœì  ì—¬ë°±ì´ ë‹¤ë¦„!
```

**ìë™ ê°ì§€ ë°©ë²•:**
```python
def detect_aspect_ratio(image):
    width, height = image.size
    ratio = width / height

    # Tolerance 5% (ì‹¤ì œ ì¹´ë©”ë¼ëŠ” ì •í™•í•˜ì§€ ì•ŠìŒ)
    if 0.95 <= ratio <= 1.05:
        return "1:1"  # Square (Instagram)
    elif 1.25 <= ratio <= 1.40:
        return "4:3"  # Standard camera
    elif 0.70 <= ratio <= 0.80:
        return "3:4"  # Portrait 4:3
    elif 1.70 <= ratio <= 1.85:
        return "16:9"  # Cinematic landscape
    elif 0.54 <= ratio <= 0.60:
        return "9:16"  # Full portrait
    else:
        return "custom"  # ê¸°íƒ€
```

**íŒ¨í„´ ì„¸ë¶„í™”:**
```python
# ê¸°ì¡´: cafe_indoor_medium_shot (300ì¥)
# ì‹ ê·œ: ì¢…íš¡ë¹„ë³„ ë¶„í• 

patterns = {
    "cafe_indoor_medium_shot_1:1": 80ì¥,   # ì¸ìŠ¤íƒ€ ì •ì‚¬ê°í˜•
    "cafe_indoor_medium_shot_4:3": 120ì¥,  # ê¸°ë³¸ ì¹´ë©”ë¼ (ê°€ì¥ ë§ìŒ)
    "cafe_indoor_medium_shot_9:16": 70ì¥,  # ì„¸ë¡œ ì „ì²´
    "cafe_indoor_medium_shot_16:9": 30ì¥   # ê°€ë¡œ ì‹œë„¤ë§ˆí‹±
}

# ê° ì¢…íš¡ë¹„ë³„ë¡œ ë…ë¦½ì ì¸ í†µê³„ ê³„ì‚°
for key, images in patterns.items():
    if len(images) < 20:  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        continue

    stats = calculate_statistics(images)
    # ì¢…íš¡ë¹„ë³„ ìµœì  ì—¬ë°±ì´ ë‹¤ë¦„!
```

**ê²°ê³¼ ì˜ˆì‹œ: cafe_indoor_medium_shot_9:16**
```json
{
  "theme": "cafe_indoor",
  "pose_type": "medium_shot",
  "aspect_ratio": "9:16",
  "sample_count": 70,

  "composition": {
    "margins": {
      "top": {"mean": 0.25, "std": 0.06},      // ì„¸ë¡œëŠ” ìœ„ ì—¬ë°± ë„‰ë„‰
      "bottom": {"mean": 0.45, "std": 0.08},   // ì•„ë˜ ë” ì—¬ìœ 
      "left": {"mean": 0.18, "std": 0.04},     // ì¢Œìš° ì—¬ë°± ì ìŒ
      "right": {"mean": 0.22, "std": 0.05}
    }
  }
}
```

**ì¢…íš¡ë¹„ë³„ ê¶Œì¥ ì—¬ë°± ì°¨ì´:**
```
1:1 (Square)
â”œâ”€ ìœ„ì•„ë˜: 20-25% (ê· ë“±)
â””â”€ ì¢Œìš°: 20-25% (ê· ë“±)

4:3 (Standard)
â”œâ”€ ìœ„ì•„ë˜: 18-22%
â””â”€ ì¢Œìš°: 22-28% (ì¢Œìš° ì¡°ê¸ˆ ë” ì—¬ìœ )

9:16 (Portrait Full)
â”œâ”€ ìœ„ì•„ë˜: 25-35% (ìœ„ì•„ë˜ ë„‰ë„‰)
â””â”€ ì¢Œìš°: 15-20% (ì¢Œìš° íƒ€ì´íŠ¸)

16:9 (Landscape)
â”œâ”€ ìœ„ì•„ë˜: 15-20% (ìœ„ì•„ë˜ íƒ€ì´íŠ¸)
â””â”€ ì¢Œìš°: 30-40% (ì¢Œìš° ë„‰ë„‰, ë°°ê²½ í™œìš©)
```

**ì‹¤ì‹œê°„ ì ìš©:**
```swift
// iOS ì•±ì—ì„œ
let currentAspectRatio = detectAspectRatio(cameraFrame)
// â†’ "9:16"

let patternKey = "\(theme)_\(poseType)_\(currentAspectRatio)"
// â†’ "cafe_indoor_medium_shot_9:16"

if let pattern = patterns[patternKey] {
    // ì¢…íš¡ë¹„ì— ë§ëŠ” íŒ¨í„´ ì‚¬ìš©
    compareWithPattern(current, pattern)
} else {
    // Fallback: ì¢…íš¡ë¹„ ë¬´ì‹œí•˜ê³  í…Œë§ˆ+í¬ì¦ˆë§Œ
    let fallbackKey = "\(theme)_\(poseType)"
    compareWithPattern(current, patterns[fallbackKey])
}
```

**ì†Œìš” ì‹œê°„:** ì¦‰ì‹œ (ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°ì—ì„œ ì½ìŒ)

---

### Step 5-1: Memory-Optimized Feature Extraction Pipeline

**ë¬¸ì œ:** 2,133ì¥ Ã— (GroundingDINO + Depth + RTMPose) = ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ„í—˜

**í•´ê²°: Batch Processing + Checkpoint System**

#### ìµœì í™” ì „ëµ

**1. Batch-wise Processing**
```python
class OptimizedFeatureExtractor:
    def __init__(self, config):
        self.batch_size = config.get("processing", {}).get("batch_size", 10)
        self.checkpoint_interval = 50  # 50ì¥ë§ˆë‹¤ ì €ì¥

    def extract_all_features(self, image_paths):
        total_batches = len(image_paths) // self.batch_size

        for batch_idx in range(total_batches):
            # 1. Batch ë¡œë“œ (ë©”ëª¨ë¦¬ ì œí•œ)
            batch = image_paths[
                batch_idx * self.batch_size :
                (batch_idx + 1) * self.batch_size
            ]

            # 2. GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()

            # 3. Batch ì²˜ë¦¬
            features = self.process_batch(batch)

            # 4. ì¦‰ì‹œ ë””ìŠ¤í¬ ì €ì¥ (ë©”ëª¨ë¦¬ í•´ì œ)
            self.save_batch_features(features)

            # 5. Checkpoint ì €ì¥ (í¬ë˜ì‹œ ë³µêµ¬ìš©)
            if batch_idx % self.checkpoint_interval == 0:
                self.save_checkpoint(batch_idx)
```

**2. Checkpoint/Resume System**
```python
class CheckpointManager:
    def __init__(self, checkpoint_dir):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)

    def save_checkpoint(self, batch_idx, processed_files):
        checkpoint = {
            "batch_idx": batch_idx,
            "processed_files": processed_files,
            "timestamp": datetime.now().isoformat()
        }

        checkpoint_path = self.checkpoint_dir / "checkpoint_latest.json"
        with open(checkpoint_path, "w") as f:
            json.dump(checkpoint, f)

    def load_checkpoint(self):
        checkpoint_path = self.checkpoint_dir / "checkpoint_latest.json"
        if checkpoint_path.exists():
            with open(checkpoint_path) as f:
                return json.load(f)
        return None

    def resume_from_checkpoint(self, all_image_paths):
        checkpoint = self.load_checkpoint()
        if checkpoint is None:
            return all_image_paths, 0

        # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ì œì™¸
        processed = set(checkpoint["processed_files"])
        remaining = [p for p in all_image_paths if p not in processed]

        print(f"Resuming from batch {checkpoint['batch_idx']}")
        print(f"Already processed: {len(processed)} files")
        print(f"Remaining: {len(remaining)} files")

        return remaining, checkpoint["batch_idx"]
```

**3. Incremental Statistics (Welford's Algorithm)**
```python
class MemoryEfficientStatistics:
    """ë©”ëª¨ë¦¬ì— ëª¨ë“  ë°ì´í„°ë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  í†µê³„ ê³„ì‚°"""

    def __init__(self):
        self.count = 0
        self.mean = 0.0
        self.M2 = 0.0  # Sum of squared differences

    def update(self, new_value):
        """ìƒˆ ê°’ í•˜ë‚˜ ì¶”ê°€ (O(1) ë©”ëª¨ë¦¬)"""
        self.count += 1
        delta = new_value - self.mean
        self.mean += delta / self.count
        delta2 = new_value - self.mean
        self.M2 += delta * delta2

    def get_statistics(self):
        if self.count < 2:
            return {"mean": self.mean, "std": 0.0}

        variance = self.M2 / (self.count - 1)
        return {
            "mean": self.mean,
            "std": math.sqrt(variance),
            "count": self.count
        }

# ì‚¬ìš© ì˜ˆì‹œ
margin_stats = MemoryEfficientStatistics()
for batch_features in all_batches:
    for feature in batch_features:
        margin_stats.update(feature["margins"]["left"])

final_stats = margin_stats.get_statistics()
# â†’ {"mean": 0.22, "std": 0.05, "count": 2133}
```

**4. GPU Memory Management**
```python
def process_batch_with_memory_management(self, batch):
    # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
    torch.cuda.set_per_process_memory_fraction(0.8)  # 80%ë§Œ ì‚¬ìš©

    try:
        # ëª¨ë¸ë³„ ìˆœì°¨ ì²˜ë¦¬ (ë™ì‹œ ì‹¤í–‰ X)
        with torch.no_grad():
            # 1. GroundingDINO
            dino_results = self.grounding_dino.detect_batch(batch)
            torch.cuda.empty_cache()

            # 2. Depth Anything
            depth_results = self.depth_model.predict_batch(batch)
            torch.cuda.empty_cache()

            # 3. RTMPose (ì´ë¯¸ ì™„ë£Œëœ ê²½ìš° ìŠ¤í‚µ)
            pose_results = self.load_pose_from_classification(batch)

        return self.combine_features(dino_results, depth_results, pose_results)

    except RuntimeError as e:
        if "out of memory" in str(e):
            # Batch size ì¤„ì—¬ì„œ ì¬ì‹œë„
            print(f"OOM! Reducing batch size from {len(batch)}")
            torch.cuda.empty_cache()
            return self.process_batch_one_by_one(batch)
        raise
```

**5. Progress Tracking**
```python
from tqdm import tqdm

def extract_all_with_progress(self, image_paths):
    progress_bar = tqdm(
        total=len(image_paths),
        desc="Feature Extraction",
        unit="images"
    )

    for batch in self.batch_iterator(image_paths):
        features = self.process_batch(batch)
        self.save_features(features)

        progress_bar.update(len(batch))
        progress_bar.set_postfix({
            "GPU": f"{torch.cuda.memory_allocated()/1e9:.2f}GB",
            "Batch": f"{len(batch)}"
        })

    progress_bar.close()
```

**ê²°ê³¼:**
```
âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 4GB ì´í•˜ ìœ ì§€ (RTX 4070 Super 12GB)
âœ… í¬ë˜ì‹œ ë³µêµ¬: Checkpointì—ì„œ ìë™ ì¬ê°œ
âœ… ì§„í–‰ ìƒí™©: ì‹¤ì‹œê°„ í‘œì‹œ (tqdm)
âœ… ì²˜ë¦¬ ì†ë„: 2,133ì¥ ì•½ 2-3ì‹œê°„
âœ… ë””ìŠ¤í¬ I/O: ë°°ì¹˜ë§ˆë‹¤ ì €ì¥ (ì •ë³´ ì†ì‹¤ 0%)
```

**ì‹¤í–‰ ëª…ë ¹:**
```bash
python extract_features_optimized.py \
    --config config.yaml \
    --batch-size 10 \
    --checkpoint-dir ./checkpoints \
    --resume  # ì¤‘ë‹¨ëœ ê²½ìš° ìë™ ì¬ê°œ
```

**ì†Œìš” ì‹œê°„:** 2-3ì‹œê°„ (2,133ì¥ ê¸°ì¤€, RTX 4070 Super)

---

### Step 5-2: Three-Tier Confidence System

**ëª©ì :** ê° íŠ¹ì§•ì˜ ì‹ ë¢°ë„ë¥¼ ì •ì§í•˜ê²Œ í‘œí˜„

**ì™œ í•„ìš”í•œê°€?**
```
ëª¨ë“  íŠ¹ì§•ì´ ë™ì¼í•œ ì‹ ë¢°ë„ë¥¼ ê°€ì§€ëŠ” ê²ƒì€ ì•„ë‹˜!

âœ… 100% í™•ì‹¤: ì¸ë¬¼ ìœ„ì¹˜, bbox í¬ê¸°
âš ï¸  70-90% ìœ ìš©: ì••ì¶•ê° ì¶”ì • (EXIF ì—†ì´)
â“  50-70% ì‹¤í—˜ì : ì¹´ë©”ë¼ ê°ë„ ì¶”ì • (ì‹¤ì‹œê°„ì€ ìì´ë¡œìŠ¤ì½”í”„ ì‚¬ìš©)
```

#### Tier 1: Certain Features (95%+ ì‹ ë¢°ë„)

**í•­ìƒ ì •í™•í•œ íŠ¹ì§•ë“¤:**
```python
certain_features = {
    "aspect_ratio": "9:16",  # ì´ë¯¸ì§€ í¬ê¸°ë¡œ 100% ì •í™•

    "position": {
        "center_x": 0.35,  # bbox ì¤‘ì‹¬ì  (í”½ì…€ ì¢Œí‘œ)
        "center_y": 0.42
    },

    "margins": {  # bbox ê¸°ë°˜ ì—¬ë°± ê³„ì‚°
        "top": 0.18,
        "bottom": 0.38,
        "left": 0.22,
        "right": 0.46
    },

    "size": {
        "bbox_ratio": 0.32,  # (bbox ë©´ì ) / (ì´ë¯¸ì§€ ë©´ì )
        "width_ratio": 0.56,
        "height_ratio": 0.58
    },

    "confidence_level": "certain"
}
```

**ì‚¬ìš©ì²˜:**
- ì‹¤ì‹œê°„ ìœ„ì¹˜ í”¼ë“œë°± ("â† ì™¼ìª½ìœ¼ë¡œ")
- í¬ê¸° í”¼ë“œë°± ("ì¡°ê¸ˆ ë” ê°€ê¹Œì´")
- ì—¬ë°± í”¼ë“œë°± ("ìœ„ ì—¬ë°± ë” í•„ìš”")

#### Tier 2: Useful Features (70-90% ì‹ ë¢°ë„)

**ë‹¤ì¤‘ ì‹ í˜¸ë¡œ ì¶”ì • ê°€ëŠ¥:**
```python
useful_features = {
    "compression": {
        "depth_compression": 0.45,  # Depth map ê¸°ë°˜
        "background_blur": 0.32,    # Blur detection
        "person_scale": 0.58,       # Person/Image ratio

        # ì¢…í•© íŒì •
        "estimated_type": "normal_to_tele",  # wide/normal/tele
        "focal_length_estimate": "50-70mm",

        "confidence_level": "useful",
        "confidence_score": 0.75  # 75% ì‹ ë¢°ë„
    },

    "background_objects": {
        "window": {
            "detected": true,
            "bbox": [720, 300, 300, 600],
            "direction": "right",
            "distance": 0.25,
            "confidence": 0.82  # GroundingDINO ì‹ ë¢°ë„
        }
    }
}
```

**ì‹ ë¢°ë„ ê³„ì‚°:**
```python
def estimate_compression_confidence(depth_map, blur_score, person_scale):
    signals = []

    # Signal 1: Depth range
    depth_range = depth_map.max() - depth_map.min()
    if depth_range > 150:
        signals.append(("wide", 0.8))
    elif depth_range < 80:
        signals.append(("tele", 0.7))
    else:
        signals.append(("normal", 0.6))

    # Signal 2: Background blur
    if blur_score > 0.5:
        signals.append(("tele", 0.6))

    # Signal 3: Person scale
    if person_scale > 0.4:
        signals.append(("tele", 0.5))

    # Voting + Confidence
    if len(signals) >= 2 and all(s[0] == signals[0][0] for s in signals):
        # ëª¨ë“  ì‹ í˜¸ ì¼ì¹˜ â†’ ë†’ì€ ì‹ ë¢°ë„
        return signals[0][0], 0.85
    else:
        # ì‹ í˜¸ ë¶ˆì¼ì¹˜ â†’ ë‚®ì€ ì‹ ë¢°ë„
        return signals[0][0], 0.60
```

**ì‚¬ìš©ì²˜:**
- ê±°ë¦¬ í”¼ë“œë°± ("1.2m ë’¤ë¡œ ë˜ëŠ” ì¤Œ ì¸")
- ë°°ê²½ í™œìš© íŒ ("ì°½ë¬¸ì„ ì˜¤ë¥¸ìª½ì— ë°°ì¹˜")

#### Tier 3: Experimental Features (50-70% ì‹ ë¢°ë„)

**ì‹¤í—˜ì  ì¶”ì •:**
```python
experimental_features = {
    "estimated_angle": {
        "pitch": 12,  # Pose keypoint ê¸°ìš¸ê¸°ë¡œ ì¶”ì •
        "method": "keypoint_ratio",
        "confidence_level": "experimental",
        "confidence_score": 0.55,

        "note": "ì˜¤í”„ë¼ì¸ í•™ìŠµìš©. ì‹¤ì‹œê°„ì€ ìì´ë¡œìŠ¤ì½”í”„ ì‚¬ìš© (95% ì‹ ë¢°ë„)"
    }
}
```

**ì™œ ë‚®ì€ê°€?**
```
ë¬¸ì œ: EXIF ë°ì´í„° ì—†ìŒ
â†’ ì´ˆì ê±°ë¦¬, ì‹¤ì œ ê±°ë¦¬ ì•Œ ìˆ˜ ì—†ìŒ
â†’ ê°ë„ ì¶”ì • ë¶€ì •í™•

í•´ê²° (ì‹¤ì‹œê°„):
â†’ iPhone ìì´ë¡œìŠ¤ì½”í”„ ì‚¬ìš©
â†’ 95% ì •í™•ë„ ë‹¬ì„±
```

**ì‚¬ìš©ì²˜:**
- ì˜¤í”„ë¼ì¸: í†µê³„ ë³´ì¡° ìë£Œ
- ì‹¤ì‹œê°„: ìì´ë¡œìŠ¤ì½”í”„ë¡œ ëŒ€ì²´

#### íŒ¨í„´ JSONì— ì‹ ë¢°ë„ í¬í•¨

```json
{
  "cafe_indoor_medium_shot_9:16": {
    "composition": {
      "position": {
        "mean": [0.35, 0.42],
        "confidence": "certain",
        "confidence_score": 1.0
      }
    },

    "camera": {
      "compression_index": {
        "mean": 0.45,
        "std": 0.12,
        "confidence": "useful",
        "confidence_score": 0.75,
        "note": "ë‹¤ì¤‘ ì‹ í˜¸ ê¸°ë°˜ ì¶”ì • (depth + blur + scale)"
      },

      "angle": {
        "mean": 12,
        "confidence": "experimental",
        "confidence_score": 0.55,
        "note": "ì˜¤í”„ë¼ì¸ í•™ìŠµìš©. ì‹¤ì‹œê°„ì€ ìì´ë¡œìŠ¤ì½”í”„ ì‚¬ìš©"
      }
    }
  }
}
```

**ì‹¤ì‹œê°„ í™œìš© ì „ëµ:**
```swift
// iOS ì•±
func generateFeedback(current: Features, pattern: Pattern) {
    var feedback: [FeedbackItem] = []

    // Tier 1: í•­ìƒ ì‚¬ìš©
    if current.position.x < pattern.position.mean.x - 0.05 {
        feedback.append(.position("â† ì™¼ìª½ìœ¼ë¡œ"))
    }

    // Tier 2: ì‹ ë¢°ë„ ì²´í¬
    if pattern.camera.compression.confidence_score > 0.7 {
        let compressionDiff = current.compression - pattern.compression.mean
        if abs(compressionDiff) > 0.15 {
            feedback.append(.distance("1.2m ë’¤ë¡œ ë˜ëŠ” ì¤Œ ì¸"))
        }
    }

    // Tier 3: ì‹¤ì‹œê°„ ì„¼ì„œë¡œ ëŒ€ì²´
    let gyroAngle = motionManager.currentAngle  // ìì´ë¡œìŠ¤ì½”í”„
    if abs(gyroAngle - pattern.angle.mean) > 5 {
        feedback.append(.angle("ì¹´ë©”ë¼ 5ë„ ìœ„ë¡œ"))
    }

    return feedback
}
```

**ê²°ë¡ :**
```
âœ… Certain (100%): ìœ„ì¹˜, í¬ê¸°, ì—¬ë°± â†’ í•µì‹¬ í”¼ë“œë°±
âœ… Useful (75%): ì••ì¶•ê°, ë°°ê²½ â†’ ë³´ì¡° í”¼ë“œë°±
âš ï¸  Experimental (55%): ê°ë„ â†’ ì‹¤ì‹œê°„ ì„¼ì„œë¡œ ëŒ€ì²´

â†’ ì •ì§í•œ ì‹ ë¢°ë„ í‘œí˜„ìœ¼ë¡œ ì‚¬ìš©ì ì‹ ë¢° í™•ë³´
â†’ í™•ì‹¤í•œ ê²ƒë§Œ ê°•ì¡°, ë¶ˆí™•ì‹¤í•œ ê²ƒì€ ë³´ì¡°
```

---

### Step 5-3: ë‘ ê°€ì§€ ì¶œë ¥ í˜•ì‹

**ëª©ì :** í•™ìŠµìš© í’€ ë°ì´í„° vs ì•±ìš© ê²½ëŸ‰ JSON ë¶„ë¦¬

#### ì¶œë ¥ 1: patterns_full_v1.json (2-5MB)

**ìš©ë„:** ì˜¤í”„ë¼ì¸ ë¶„ì„, ë””ë²„ê¹…, ê°œì„ 

**í¬í•¨ ë‚´ìš©:**
```json
{
  "cafe_indoor_medium_shot_9:16": {
    "meta": {
      "theme": "cafe_indoor",
      "pose_type": "medium_shot",
      "aspect_ratio": "9:16",
      "sample_count": 70,
      "created_at": "2025-12-01T15:30:00"
    },

    "composition": {
      "position": {
        "mean": [0.35, 0.42],
        "std": [0.08, 0.10],
        "median": [0.34, 0.41],
        "percentile_25": [0.28, 0.35],
        "percentile_75": [0.42, 0.48],
        "outliers_removed": 3
      }
    },

    "raw_statistics": {
      "all_positions": [...],  // ë””ë²„ê¹…ìš©
      "all_compression_indices": [...]
    },

    "exemplars": [
      {
        "filename": "IMG_1234.jpg",
        "score": 98,
        "features": {...}  // ì „ì²´ íŠ¹ì§•
      }
    ]
  }
}
```

#### ì¶œë ¥ 2: patterns_app_v1.json (~50KB)

**ìš©ë„:** iOS ì•± ë‚´ì¥ (ìš©ëŸ‰ ìµœì†Œí™”)

**í¬í•¨ ë‚´ìš©:**
```json
{
  "cafe_indoor_medium_shot_9:16": {
    "composition": {
      "position": {"mean": [0.35, 0.42], "std": [0.08, 0.10]},
      "size": {"mean": 0.32, "range": [0.28, 0.38]},
      "margins": {
        "left": {"mean": 0.22, "std": 0.05},
        "right": {"mean": 0.46, "std": 0.08},
        "top": {"mean": 0.18, "std": 0.06},
        "bottom": {"mean": 0.38, "std": 0.10}
      }
    },

    "camera": {
      "compression": {
        "mean": 0.45,
        "range": [0.30, 0.65],
        "confidence": 0.75
      }
    },

    "weights": {
      "position": 0.20,
      "size": 0.15,
      "margin": 0.15,
      "compression": 0.20,
      "pose": 0.15,
      "angle": 0.15
    }
  }
}
```

**í¬ê¸° ë¹„êµ:**
```
patterns_full_v1.json:     2.3 MB (ëª¨ë“  í†µê³„ + ì›ë³¸ ë°ì´í„°)
patterns_app_v1.json:      48 KB (í•„ìˆ˜ í†µê³„ë§Œ)

â†’ ì•± í¬ê¸° ì˜í–¥ ìµœì†Œí™” (50KB ì¶”ê°€)
â†’ ë„¤íŠ¸ì›Œí¬ ë‹¤ìš´ë¡œë“œ ë¶ˆí•„ìš” (ì•± ë‚´ì¥)
```

**ìƒì„± ì½”ë“œ:**
```python
def generate_pattern_outputs(full_patterns):
    # Full version (í•™ìŠµìš©)
    with open("patterns_full_v1.json", "w") as f:
        json.dump(full_patterns, f, indent=2)

    # Lightweight version (ì•±ìš©)
    app_patterns = {}
    for key, pattern in full_patterns.items():
        app_patterns[key] = {
            "composition": {
                "position": {
                    "mean": pattern["composition"]["position"]["mean"],
                    "std": pattern["composition"]["position"]["std"]
                },
                "size": {
                    "mean": pattern["composition"]["size"]["mean"],
                    "range": pattern["composition"]["size"]["range"]
                },
                "margins": pattern["composition"]["margins"]
            },
            "camera": {
                "compression": {
                    "mean": pattern["camera"]["compression"]["mean"],
                    "range": pattern["camera"]["compression"]["range"],
                    "confidence": pattern["camera"]["compression"]["confidence_score"]
                }
            },
            "weights": pattern["scoring_weights"]
        }

    with open("patterns_app_v1.json", "w") as f:
        json.dump(app_patterns, f, indent=2)

    print(f"Full JSON: {os.path.getsize('patterns_full_v1.json') / 1024:.1f} KB")
    print(f"App JSON: {os.path.getsize('patterns_app_v1.json') / 1024:.1f} KB")
```

**ì†Œìš” ì‹œê°„:** ì¦‰ì‹œ (í†µê³„ ê³„ì‚° ì™„ë£Œ í›„)

---

### Step 6: Hierarchical Pattern í†µê³„ ê³„ì‚°

**í•µì‹¬:** Theme + Pose Type ì¡°í•©ìœ¼ë¡œ ì„¸ë¶„í™”!

**ì™œ?** ì•‰ì€ ìƒë°˜ì‹ ê³¼ ì„œìˆëŠ” ì „ì‹ ì„ ì„ìœ¼ë©´ í†µê³„ê°€ ë¬´ì˜ë¯¸í•´ì§

**ì˜ˆì‹œ: cafe_indoor (668ì¥) â†’ 4ê°œ sub-patternìœ¼ë¡œ ë¶„í• **

```python
cafe_indoor_images = filter_by_theme("cafe_indoor")  # 668ì¥

# Pose Typeë³„ ë¶„í• 
patterns = {
    "cafe_indoor_closeup": [],      # 50ì¥
    "cafe_indoor_upper_body": [],   # 200ì¥
    "cafe_indoor_half_body": [],    # 300ì¥ (ê°€ì¥ ë§ìŒ!)
    "cafe_indoor_full_body": []     # 118ì¥
}

for img in cafe_indoor_images:
    key = f"{img.theme}_{img.pose_type}"
    patterns[key].append(img)

# ê° sub-patternë³„ë¡œ í†µê³„ ê³„ì‚°
for pattern_name, images in patterns.items():
    if len(images) < 30:  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ìƒí–¥
        continue

    stats = calculate_statistics(images)
    save_pattern(pattern_name, stats)
```

**ê²°ê³¼: cafe_indoor_half_body (300ì¥)**

```json
{
  "theme": "cafe_indoor",
  "pose_type": "half_body",
  "sample_count": 300,

  "position": {
    "mean": [0.35, 0.42],
    "std": [0.08, 0.10],
    "optimal_range": {
      "x": [0.25, 0.45],
      "y": [0.30, 0.55]
    }
  },

  "size": {
    "mean": 0.32,
    "std": 0.06,
    "optimal_range": [0.28, 0.38]
  },

  "margins": {
    "left": {"mean": 0.22, "std": 0.05},
    "right": {"mean": 0.46, "std": 0.08},
    "asymmetry": "right_heavy"
  },

  "camera": {
    "compression_index": {
      "mean": 0.45,
      "std": 0.12
    },
    "type": "normal_to_tele",
    "focal_length_estimate": "50-70mm",
    "angle": {
      "mean": 12,
      "std": 5,
      "range": [5, 20]
    }
  },

  "pose_requirements": {
    "sitting": true,
    "visible_joints": ["shoulders", "elbows", "hips", "knees"]
  },

  "background": {
    "window": {
      "presence_rate": 0.78,
      "typical_direction": "right",
      "distance_from_person": {"mean": 0.25}
    }
  }
}
```

**ì¤‘ìš”:** ì´ì œ "cafe ë°˜ì‹  ì•‰ì€ ì‚¬ì§„"ë§Œ ë¹„êµí•˜ë¯€ë¡œ í†µê³„ê°€ ì˜ë¯¸ìˆìŒ!

---

## ğŸ’¾ íŒ¨í„´ DB êµ¬ì¡° (JSON)

**Hierarchical Structure: theme â†’ pose_type â†’ sub-patterns**

```json
{
  "cafe_indoor": {
    "theme_description": "ì¹´í˜ ì‹¤ë‚´",
    "total_samples": 668,

    "sub_patterns": {
      "cafe_indoor_half_body": {
        "theme": "cafe_indoor",
        "pose_type": "half_body",
        "sample_count": 300,
        "description": "ì¹´í˜ ë°˜ì‹  (ê°€ì¥ ì¸ê¸°)",

        "composition": {
          "position": {
            "mean": [0.35, 0.42],
            "std": [0.08, 0.10],
            "acceptable_range": {
              "x": [0.25, 0.45],
              "y": [0.30, 0.55]
            }
          },
          "size": {
            "mean": 0.32,
            "optimal_range": [0.28, 0.38]
          },
          "margins": {
            "left": {"mean": 0.22, "std": 0.05},
            "right": {"mean": 0.46, "std": 0.08},
            "top": {"mean": 0.18, "std": 0.06},
            "bottom": {"mean": 0.38, "std": 0.10}
          }
        },

        "camera": {
          "angle": {
            "mean": 12,
            "std": 5,
            "range": [5, 20],
            "interpretation": "slight_high_angle"
          },
          "compression_index": {
            "mean": 0.45,
            "std": 0.12,
            "range": [0.30, 0.65]
          },
          "type": "normal_to_tele",
          "focal_length_estimate": "50-70mm"
        },

        "pose_requirements": {
          "sitting": true,
          "visible_joints": ["shoulders", "elbows", "hips", "knees"],
          "min_confidence": 0.3
        },

        "background": {
          "required_objects": ["window"],
          "common_objects": ["chair", "table", "plant"],
          "window": {
            "presence_rate": 0.78,
            "typical_direction": "right",
            "distance_from_person": {"mean": 0.25, "std": 0.08}
          }
        },

        "scoring_weights": {
          "position_match": 0.20,
          "size_match": 0.15,
          "margin_match": 0.15,
          "compression_match": 0.20,
          "angle_match": 0.15,
          "pose_match": 0.15
        },

        "exemplars": [
          {"filename": "IMG_1234.jpg", "score": 98},
          {"filename": "IMG_2345.jpg", "score": 96}
        ]
      },

      "cafe_indoor_upper_body": {
        "theme": "cafe_indoor",
        "pose_type": "upper_body",
        "sample_count": 200,
        "description": "ì¹´í˜ ìƒë°˜ì‹  í´ë¡œì¦ˆì—…",

        "composition": {
          "position": {
            "mean": [0.48, 0.38],
            "std": [0.06, 0.08]
          }
        }
      }
    }
  },

  "park_nature": {
    "theme_description": "ê³µì›/ìì—°",
    "total_samples": 908,

    "sub_patterns": {
      "park_full_body": {...},
      "park_half_body": {...}
    }
  }
}
```

**í•µì‹¬ ê°œì„ :**
- âœ… Theme + Pose Type ì¡°í•©ìœ¼ë¡œ ì •ë°€ ë¶„ë¥˜
- âœ… ì•‰ìŒ/ì„œìˆìŒ êµ¬ë¶„
- âœ… ì¹´ë©”ë¼ ê°ë„ í¬í•¨ (gyroscope ê¸°ë°˜)
- âœ… Pose requirements ëª…ì‹œ

---

## ğŸ“± Phase 2: ì‹¤ì‹œê°„ í”¼ë“œë°± ì‹œìŠ¤í…œ

### ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ (iPhone)

**ëª©í‘œ ì„±ëŠ¥:** 15fps (66ms/frame)

**ì£¼ì˜:** RTMPose = YOLOX (detector) + RTMPose (pose estimator)

```
í”„ë ˆì„ ìº¡ì²˜
  â†“
YOLOX person bbox (5ms)
  â†“
RTMPose keypoints (5ms)
  â†“
Pose Type íŒì • (1ms)
  â†“
Gyroscope ê°ë„ ì½ê¸° (ì¦‰ì‹œ, ë°±ê·¸ë¼ìš´ë“œ)
  â†“
MiDaS Small depth ì¶”ì • (50ms)
  â†“
êµ¬ë„ íŠ¹ì§• ê³„ì‚° (2ms)
  â†“
íŒ¨í„´ ë§¤ì¹­ (í˜„ì¬ pose_typeì— ë§ëŠ” íŒ¨í„´ê³¼ ë¹„êµ, 3ms)
  â†“
3ë‹¨ê³„ í”¼ë“œë°± ìƒì„± (1ms)
  â†“
UI ì—…ë°ì´íŠ¸

ì´: 67ms â†’ 15fps âœ…
```

**ì¤‘ìš”: íŒ¨í„´ ë¡œë”© ìµœì í™”**
```swift
class PatternManager {
    private static var cachedPatterns: [String: Pattern] = [:]

    static func loadPatterns() {
        // ì•± ì‹œì‘ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ
        if let path = Bundle.main.path(forResource: "patterns_app_v1", ofType: "json") {
            let data = try! Data(contentsOf: URL(fileURLWithPath: path))
            cachedPatterns = try! JSONDecoder().decode([String: Pattern].self, from: data)
        }
    }

    static func getPattern(theme: String, poseType: String, aspectRatio: String) -> Pattern? {
        let key = "\(theme)_\(poseType)_\(aspectRatio)"
        if let pattern = cachedPatterns[key] {
            return pattern
        }

        // Fallback: ì¢…íš¡ë¹„ ë¬´ì‹œ
        let fallbackKey = "\(theme)_\(poseType)"
        return cachedPatterns[fallbackKey]
    }
}
```

**RTMPose ìƒì„¸:**
```swift
// RTMPoseRunner.swift ì´ë¯¸ êµ¬í˜„ë¨
let result = rtmPoseRunner.run(image)

// result.boundingBox: YOLOXê°€ ê²€ì¶œí•œ person bbox
// result.keypoints: RTMPoseê°€ ì¶”ì •í•œ 133ê°œ keypoints
// result.confidences: ê° keypoint ì‹ ë¢°ë„

// Pose Type íŒì •
let poseType = determinePoseType(
    keypoints: result.keypoints,
    confidences: result.confidences
)
// â†’ "half_body", "upper_body", "full_body", "closeup"
```

### 3ë‹¨ê³„ í”¼ë“œë°± ì‹œìŠ¤í…œ

#### Tier 1: ìœ„ì¹˜ í”¼ë“œë°± (ì¦‰ê°)
```
current.position.x < ideal.position.x - 0.05
â†’ "â† ì™¼ìª½ìœ¼ë¡œ"

current.position.x > ideal.position.x + 0.05
â†’ "â†’ ì˜¤ë¥¸ìª½ìœ¼ë¡œ"

else
â†’ "âœ“ ìœ„ì¹˜ ì™„ë²½"
```

#### Tier 2: ê±°ë¦¬ í”¼ë“œë°± (í•µì‹¬!)
```
compression_diff = current.compression - ideal.compression

if compression_diff > 0.15:  // ë„ˆë¬´ ë§ì›
    distance = estimate_distance(compression_diff)
    â†’ "ğŸ“ {distance}m ë” ê°€ê¹Œì´"
    â†’ "ë˜ëŠ” ì¤Œ ì•„ì›ƒ"

elif compression_diff < -0.15:  // ë„ˆë¬´ ê´‘ê°
    distance = estimate_distance(compression_diff)
    â†’ "ğŸ“ {distance}m ë’¤ë¡œ"
    â†’ "ë˜ëŠ” ì¤Œ ì¸"

else:
    â†’ "âœ“ ê±°ë¦¬ ì™„ë²½"
```

#### Tier 3: ì¢…í•© ì ìˆ˜
```
overall_score = weighted_average(
    position_match,
    size_match,
    margin_match,
    compression_match
)

if score >= 95:
    â†’ "ğŸ‰ ì™„ë²½! ì´¬ì˜í•˜ì„¸ìš”"
    + í–…í‹± í”¼ë“œë°±
    + ì´ˆë¡ìƒ‰ í…Œë‘ë¦¬

elif score >= 85:
    â†’ "ğŸ‘ í›Œë¥­í•©ë‹ˆë‹¤ ({score}/100)"

else:
    â†’ "ì¡°ì • í•„ìš” ({score}/100)"
```

---

## ğŸ“± Phase 2.5: iOS ì•± í†µí•© ì „ëµ

### í˜„ì¬ ì•± êµ¬ì¡° ë¶„ì„ (main ë¸Œëœì¹˜)

**ê¸°ì¡´ ì‹œìŠ¤í…œ: Rule-Based Approach**

```swift
// RealtimeAnalyzer.swift (ê¸°ì¡´)
class RealtimeAnalyzer: ObservableObject {
    func analyzeFrame(_ image: UIImage) {
        // 1. Vision Framework - ì–¼êµ´ ê²€ì¶œë§Œ
        let faceRect = detectFace(image)  // VNDetectFaceRectanglesRequest

        // 2. ë‹¨ìˆœ ì¶”ì • (ë§¤ìš° ë¶€ì •í™•!)
        let bodyRect = estimateBodyRect(from: faceRect)
        // bodyHeight = faceHeight Ã— 7 (ê³ ì • ë¹„ìœ¨)

        // 3. ë ˆí¼ëŸ°ìŠ¤ì™€ ë‹¨ìˆœ ë¹„êµ
        let zoomDiff = abs(currentBodyHeight - referenceBodyHeight)
        if zoomDiff > 10 {
            feedback.append("ğŸ” ì¤Œ ì¸/ì•„ì›ƒ")
        }
    }

    private func estimateBodyRect(from faceRect: CGRect?) -> CGRect? {
        guard let face = faceRect else { return nil }
        let bodyHeight = face.height * 7  // ë§¤ìš° ë¶€ì •í™•!
        let bodyWidth = face.width * 3
        return CGRect(x: face.minX, y: face.minY,
                      width: bodyWidth, height: bodyHeight)
    }
}
```

**ê¸°ì¡´ ì‹œìŠ¤í…œì˜ í•œê³„:**
```
âŒ ì–¼êµ´ ê¸°ë°˜ ì¶”ì •ë§Œ (ì „ì‹  ì‚¬ì§„ ë¶ˆê°€)
âŒ ê³ ì • ë¹„ìœ¨ (face Ã— 7) - ì•‰ìŒ/ì„œìˆìŒ êµ¬ë¶„ ë¶ˆê°€
âŒ ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ í•„ìš” (ì‚¬ìš©ìê°€ ì§ì ‘ ì„ íƒ)
âŒ í…Œë§ˆë³„ íŒ¨í„´ ì—†ìŒ
âŒ ì—¬ë°±/êµ¬ë„ ë¶„ì„ ì—†ìŒ
âŒ ì••ì¶•ê° ê³ ë ¤ ì—†ìŒ
```

### ìƒˆ ì‹œìŠ¤í…œ: Hybrid Pattern-Based Approach

**í•µì‹¬ ì „ëµ: ê¸°ì¡´ ì½”ë“œ í™œìš© + íŒ¨í„´ ê°•í™”**

```swift
// PatternBasedAnalyzer.swift (ì‹ ê·œ)
class PatternBasedAnalyzer: ObservableObject {
    private let patternManager = PatternManager.shared
    private let rtmPoseRunner: RTMPoseRunner  // ì´ë¯¸ êµ¬í˜„ë¨
    private let depthEstimator: DepthEstimator  // MiDaS Small

    func analyzeFrame(_ image: UIImage, theme: String) {
        // 1. RTMPoseë¡œ ì •ë°€ ê²€ì¶œ (ê¸°ì¡´ ì½”ë“œ í™œìš©)
        let poseResult = rtmPoseRunner.run(image)
        // â†’ bbox, keypoints, confidence

        // 2. Pose Type ìë™ íŒì •
        let poseType = determinePoseType(
            keypoints: poseResult.keypoints,
            bbox: poseResult.boundingBox
        )
        // â†’ "closeup", "medium_shot", "knee_shot", "full_shot"

        // 3. Aspect Ratio ê°ì§€
        let aspectRatio = detectAspectRatio(image.size)
        // â†’ "9:16", "4:3", "1:1", etc.

        // 4. íŒ¨í„´ ë¡œë“œ
        guard let pattern = patternManager.getPattern(
            theme: theme,
            poseType: poseType,
            aspectRatio: aspectRatio
        ) else {
            // Fallback to rule-based
            return fallbackToRuleBased(poseResult)
        }

        // 5. í˜„ì¬ í”„ë ˆì„ íŠ¹ì§• ì¶”ì¶œ
        let currentFeatures = extractFeatures(
            image: image,
            bbox: poseResult.boundingBox,
            depth: depthEstimator.estimate(image)
        )

        // 6. íŒ¨í„´ ë§¤ì¹­ & í”¼ë“œë°± ìƒì„±
        let feedback = generatePatternBasedFeedback(
            current: currentFeatures,
            pattern: pattern,
            gyroAngle: motionManager.currentAngle
        )

        // 7. UI ì—…ë°ì´íŠ¸
        updateFeedback(feedback)
    }
}
```

### í•µì‹¬ í•¨ìˆ˜ êµ¬í˜„

#### 1. Pose Type íŒì • (ê¸°ì¡´ RTMPose í™œìš©)

```swift
func determinePoseType(keypoints: [CGPoint], bbox: CGRect) -> String {
    let imageHeight = UIScreen.main.bounds.height

    // ì‹ ë¢°ë„ ìˆëŠ” keypointë§Œ ì‚¬ìš©
    let confidences = rtmPoseResult.confidences
    let threshold: Float = 0.3

    // ì£¼ìš” ê´€ì ˆ ê°€ì‹œì„± ì²´í¬
    let shoulderVisible = confidences[5] > threshold && confidences[6] > threshold
    let hipVisible = confidences[11] > threshold && confidences[12] > threshold
    let kneeVisible = confidences[13] > threshold && confidences[14] > threshold
    let ankleVisible = confidences[15] > threshold && confidences[16] > threshold

    // Bbox ê¸°ë°˜ ë³´ì¡° íŒë‹¨
    let bboxBottomRatio = bbox.maxY / imageHeight

    // íŒì • ë¡œì§ (v1.5 4-type ê¸°ì¤€)
    if ankleVisible && bboxBottomRatio > 0.85 {
        return "full_shot"
    } else if kneeVisible && bboxBottomRatio > 0.75 {
        return "knee_shot"
    } else if hipVisible {
        return "medium_shot"
    } else if shoulderVisible {
        // ì–¼êµ´ ë¹„ìœ¨ë¡œ closeup vs medium êµ¬ë¶„
        let faceHeight = keypoints[0].y - keypoints[3].y
        let faceRatio = faceHeight / bbox.height
        return faceRatio > 0.3 ? "closeup" : "medium_shot"
    } else {
        return "medium_shot"  // Default
    }
}
```

#### 2. í˜„ì¬ í”„ë ˆì„ íŠ¹ì§• ì¶”ì¶œ

```swift
func extractFeatures(image: UIImage, bbox: CGRect, depth: DepthMap?) -> FrameFeatures {
    let imageSize = image.size

    // Tier 1: Certain Features (100% ì‹ ë¢°ë„)
    let centerX = (bbox.midX / imageSize.width)
    let centerY = (bbox.midY / imageSize.height)

    let margins = Margins(
        top: bbox.minY / imageSize.height,
        bottom: (imageSize.height - bbox.maxY) / imageSize.height,
        left: bbox.minX / imageSize.width,
        right: (imageSize.width - bbox.maxX) / imageSize.width
    )

    let bboxRatio = (bbox.width * bbox.height) / (imageSize.width * imageSize.height)

    // Tier 2: Useful Features (75% ì‹ ë¢°ë„)
    var compressionIndex: Float = 0.5  // Default
    if let depthMap = depth {
        compressionIndex = calculateCompressionIndex(depthMap, bbox: bbox)
    }

    return FrameFeatures(
        position: CGPoint(x: centerX, y: centerY),
        margins: margins,
        size: bboxRatio,
        compression: compressionIndex,
        aspectRatio: detectAspectRatio(imageSize)
    )
}

func calculateCompressionIndex(_ depthMap: DepthMap, bbox: CGRect) -> Float {
    // Person depth
    let personDepth = depthMap.meanDepth(in: bbox)

    // Background depth (ìƒë‹¨ 1/3)
    let bgRect = CGRect(x: 0, y: 0,
                        width: depthMap.width,
                        height: depthMap.height / 3)
    let bgDepth = depthMap.meanDepth(in: bgRect)

    // Foreground depth (í•˜ë‹¨ 1/4)
    let fgRect = CGRect(x: 0, y: depthMap.height * 3 / 4,
                        width: depthMap.width,
                        height: depthMap.height / 4)
    let fgDepth = depthMap.meanDepth(in: fgRect)

    // Compression Index
    let depthRange = bgDepth - fgDepth
    let maxRange: Float = 255.0
    let compressionIndex = 1.0 - (depthRange / maxRange)

    return compressionIndex
}
```

#### 3. íŒ¨í„´ ê¸°ë°˜ í”¼ë“œë°± ìƒì„±

```swift
func generatePatternBasedFeedback(
    current: FrameFeatures,
    pattern: Pattern,
    gyroAngle: Double
) -> [FeedbackItem] {
    var feedback: [FeedbackItem] = []
    var score: Float = 0.0

    // Tier 1: ìœ„ì¹˜ í”¼ë“œë°± (confidence: 1.0)
    let positionDiff = current.position.x - pattern.composition.position.mean.x
    if abs(positionDiff) > 0.05 {
        if positionDiff < 0 {
            feedback.append(FeedbackItem(
                type: .position,
                message: "â†’ ì˜¤ë¥¸ìª½ìœ¼ë¡œ",
                priority: .high
            ))
        } else {
            feedback.append(FeedbackItem(
                type: .position,
                message: "â† ì™¼ìª½ìœ¼ë¡œ",
                priority: .high
            ))
        }
        score += 15.0  // ìœ„ì¹˜ ë¶ˆì¼ì¹˜ í˜ë„í‹°
    } else {
        score += 20.0  // ìœ„ì¹˜ ì™„ë²½ (weight: 0.20)
    }

    // Tier 1: ì—¬ë°± í”¼ë“œë°±
    let topMarginDiff = current.margins.top - pattern.composition.margins.top.mean
    if abs(topMarginDiff) > 0.08 {
        if topMarginDiff < 0 {
            feedback.append(FeedbackItem(
                type: .margin,
                message: "â†“ ì¹´ë©”ë¼ ì•„ë˜ë¡œ (ìœ„ ì—¬ë°± í•„ìš”)",
                priority: .medium
            ))
        } else {
            feedback.append(FeedbackItem(
                type: .margin,
                message: "â†‘ ì¹´ë©”ë¼ ìœ„ë¡œ (ìœ„ ì—¬ë°± ê³¼ë‹¤)",
                priority: .medium
            ))
        }
        score += 10.0
    } else {
        score += 15.0  // ì—¬ë°± ì™„ë²½
    }

    // Tier 2: ì••ì¶•ê° í”¼ë“œë°± (confidence: 0.75)
    if pattern.camera.compression.confidence > 0.7 {
        let compressionDiff = current.compression - pattern.camera.compression.mean

        if compressionDiff > 0.15 {
            // ë„ˆë¬´ ë§ì›/ì••ì¶•ë¨
            let distance = estimateDistance(compressionDiff)
            feedback.append(FeedbackItem(
                type: .distance,
                message: "ğŸ“ \(distance)m ë” ê°€ê¹Œì´ ë˜ëŠ” ì¤Œ ì•„ì›ƒ",
                priority: .medium
            ))
            score += 12.0
        } else if compressionDiff < -0.15 {
            // ë„ˆë¬´ ê´‘ê°
            let distance = estimateDistance(abs(compressionDiff))
            feedback.append(FeedbackItem(
                type: .distance,
                message: "ğŸ“ \(distance)m ë’¤ë¡œ ë˜ëŠ” ì¤Œ ì¸",
                priority: .medium
            ))
            score += 12.0
        } else {
            score += 20.0  // ì••ì¶•ê° ì™„ë²½
        }
    }

    // Tier 3: ê°ë„ í”¼ë“œë°± (ì‹¤ì‹œê°„ ìì´ë¡œìŠ¤ì½”í”„ ì‚¬ìš©)
    let angleDiff = gyroAngle - Double(pattern.camera.angle.mean)
    if abs(angleDiff) > 5.0 {
        if angleDiff < 0 {
            feedback.append(FeedbackItem(
                type: .angle,
                message: "ğŸ“ ì¹´ë©”ë¼ \(Int(abs(angleDiff)))Â° ìœ„ë¡œ",
                priority: .low
            ))
        } else {
            feedback.append(FeedbackItem(
                type: .angle,
                message: "ğŸ“ ì¹´ë©”ë¼ \(Int(abs(angleDiff)))Â° ì•„ë˜ë¡œ",
                priority: .low
            ))
        }
        score += 10.0
    } else {
        score += 15.0  // ê°ë„ ì™„ë²½
    }

    // í¬ê¸° í”¼ë“œë°±
    let sizeDiff = current.size - pattern.composition.size.mean
    if abs(sizeDiff) > 0.08 {
        if sizeDiff < 0 {
            feedback.append(FeedbackItem(
                type: .size,
                message: "ğŸ” ì¡°ê¸ˆ ë” ê°€ê¹Œì´",
                priority: .medium
            ))
        } else {
            feedback.append(FeedbackItem(
                type: .size,
                message: "ğŸ” ì¡°ê¸ˆ ë” ë©€ë¦¬",
                priority: .medium
            ))
        }
        score += 10.0
    } else {
        score += 15.0
    }

    // ì¢…í•© ì ìˆ˜
    feedback.append(FeedbackItem(
        type: .score,
        message: "â­ êµ¬ë„ ì ìˆ˜: \(Int(score))/100",
        priority: score >= 95 ? .celebration : .info
    ))

    // ì™„ë²½ ë‹¬ì„±ì‹œ í–…í‹±
    if score >= 95 {
        feedback.insert(FeedbackItem(
            type: .perfect,
            message: "ğŸ‰ ì™„ë²½! ì§€ê¸ˆ ì´¬ì˜í•˜ì„¸ìš”",
            priority: .celebration
        ), at: 0)
        triggerHapticFeedback()
    }

    return feedback
}

func estimateDistance(_ compressionDiff: Float) -> Float {
    // Compression Index ì°¨ì´ â†’ ì‹¤ì œ ê±°ë¦¬ ì¶”ì •
    // ê²½í—˜ì  ê³µì‹ (í•™ìŠµ ë°ì´í„° ê¸°ë°˜)
    // compressionDiff 0.1 â‰ˆ 0.5m
    return abs(compressionDiff) * 5.0
}
```

### í†µí•© ê³„íš

**Phase 1: ê¸°ì¡´ ì½”ë“œ ìœ ì§€í•˜ë©° íŒ¨í„´ ì¶”ê°€ (Hybrid)**

```swift
// ViewController.swift
class CameraViewController: UIViewController {
    private var usePatternBased: Bool = true  // ì„¤ì •ìœ¼ë¡œ ì „í™˜ ê°€ëŠ¥

    func processFrame(_ frame: CVPixelBuffer) {
        if usePatternBased && patternManager.hasPatterns {
            // ìƒˆ ì‹œìŠ¤í…œ
            patternBasedAnalyzer.analyzeFrame(frame, theme: currentTheme)
        } else {
            // ê¸°ì¡´ ì‹œìŠ¤í…œ (Fallback)
            realtimeAnalyzer.analyzeFrame(frame)
        }
    }
}
```

**Phase 2: ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜**

```
Week 1:
âœ… patterns_app_v1.json ì•±ì— ì¶”ê°€
âœ… PatternManager êµ¬í˜„
âœ… íŒ¨í„´ ë¡œë”© í…ŒìŠ¤íŠ¸

Week 2:
âœ… PatternBasedAnalyzer êµ¬í˜„
âœ… ê¸°ì¡´ RTMPose í†µí•©
âœ… Depth estimation ì¶”ê°€ (MiDaS)

Week 3:
âœ… í”¼ë“œë°± UI ê°œì„ 
âœ… í–…í‹± í”¼ë“œë°± ì¶”ê°€
âœ… A/B í…ŒìŠ¤íŠ¸ (Rule vs Pattern)

Week 4:
âœ… ì‚¬ìš©ì í…ŒìŠ¤íŠ¸
âœ… íŒŒë¼ë¯¸í„° íŠœë‹
âœ… ì„±ëŠ¥ ìµœì í™”
```

### Fallback ì „ëµ

```swift
func fallbackToRuleBased(_ poseResult: RTMPoseResult) {
    // íŒ¨í„´ ì—†ì„ ê²½ìš° ê¸°ì¡´ rule-basedë¡œ
    print("[Fallback] No pattern found, using rule-based")

    let bbox = poseResult.boundingBox

    // ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ í”¼ë“œë°±
    if bbox.midX < frame.width * 0.4 {
        feedback.append("â†’ ì˜¤ë¥¸ìª½ìœ¼ë¡œ")
    } else if bbox.midX > frame.width * 0.6 {
        feedback.append("â† ì™¼ìª½ìœ¼ë¡œ")
    }

    if bbox.height / frame.height < 0.3 {
        feedback.append("ğŸ” ë” ê°€ê¹Œì´")
    } else if bbox.height / frame.height > 0.7 {
        feedback.append("ğŸ” ë” ë©€ë¦¬")
    }
}
```

### ì„±ëŠ¥ ìµœì í™”

```swift
class PerformanceOptimizer {
    private var frameSkipCounter = 0
    private let skipInterval = 3  // 3í”„ë ˆì„ë§ˆë‹¤ 1ë²ˆ ë¶„ì„

    func shouldProcessFrame() -> Bool {
        frameSkipCounter += 1
        if frameSkipCounter >= skipInterval {
            frameSkipCounter = 0
            return true
        }
        return false
    }
}

// ì‚¬ìš©
if performanceOptimizer.shouldProcessFrame() {
    // Depth estimation (ë¹„ìŒˆ) - 3í”„ë ˆì„ë§ˆë‹¤
    let depth = depthEstimator.estimate(frame)
} else {
    // ë¹ ë¥¸ ë¶„ì„ë§Œ (ìœ„ì¹˜, í¬ê¸°)
    let quickFeatures = extractQuickFeatures(frame)
}
```

**ê²°ê³¼:**
```
âœ… ê¸°ì¡´ ì½”ë“œ ì¬í™œìš© (RTMPose, Vision Framework)
âœ… ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ (Hybrid ëª¨ë“œ)
âœ… Fallback ì§€ì› (íŒ¨í„´ ì—†ì„ ë•Œ)
âœ… ì„±ëŠ¥ ìµœì í™” (Frame skip, ìºì‹±)
âœ… A/B í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ (Rule vs Pattern)
```

---

## ğŸ¨ UI ë ˆì´ì•„ì›ƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                 â”‚
â”‚    [ì¹´ë©”ë¼ ë·°íŒŒì¸ë”]            â”‚
â”‚                                 â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚         â”‚   ğŸ‘¤    â”‚             â”‚ â† YOLO bbox ì˜¤ë²„ë ˆì´
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                 â”‚
â”‚    â”Œâ”€ â”€ â”€ â”€ â”€ â”€ â”€â”            â”‚ â† ì´ìƒì  ìœ„ì¹˜ (ì ì„ )
â”‚                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â† ì™¼ìª½ìœ¼ë¡œ                     â”‚ â† Tier 1
â”‚  ğŸ“ 1.2m ë’¤ë¡œ ë˜ëŠ” ì¤Œ ì¸        â”‚ â† Tier 2 (í•µì‹¬!)
â”‚  â­ êµ¬ë„ ì ìˆ˜: 87/100          â”‚ â† Tier 3
â”‚                                 â”‚
â”‚  [ì¹´í˜ ì°½ê°€ íŒ¨í„´]               â”‚
â”‚  [ëŒ€í‘œ ì˜ˆì‹œ ë³´ê¸°]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ í•µì‹¬ ì°¨ë³„ì 

### ì••ì¶•ê° ê¸°ë°˜ ê±°ë¦¬ í”¼ë“œë°± (ì„¸ê³„ ìµœì´ˆ)

**ë¬¸ì œ:**
```
ì‚¬ì§„ A: ê°€ê¹Œì´ì„œ ê´‘ê° (24mm)
ì‚¬ì§„ B: ë©€ë¦¬ì„œ ë§ì› (85mm)

â†’ ë‘ ì‚¬ì§„ ëª¨ë‘ ì¸ë¬¼ í¬ê¸° ê°™ì„ ìˆ˜ ìˆìŒ
â†’ í•˜ì§€ë§Œ ì™„ì „íˆ ë‹¤ë¥¸ ëŠë‚Œ!
```

**í•´ê²°:**
```
Depth Mapìœ¼ë¡œ ì••ì¶•ê° ì¸¡ì •
â†’ "1.2m ë’¤ë¡œ" ì •ëŸ‰ì  í”¼ë“œë°±
â†’ "ì¸ìŠ¤íƒ€ ëŠë‚Œ"ì˜ í•µì‹¬ ì¬í˜„
```

### ê¸°ì¡´ ì•± vs TryAngle v1.5

| ê¸°ëŠ¥ | ê¸°ì¡´ ì•± | TryAngle v1.5 |
|------|---------|---------------|
| ìœ„ì¹˜ ê°€ì´ë“œ | "ê·¸ë¦¬ë“œ ë¼ì¸" | "ì™¼ìª½ìœ¼ë¡œ 10cm" |
| ê±°ë¦¬ ê°€ì´ë“œ | âŒ ì—†ìŒ | "1.2m ë’¤ë¡œ ë˜ëŠ” ì¤Œ ì¸" â­ |
| ì ìˆ˜ | âŒ ì—†ìŒ | "87/100, 3ì ë§Œ ë”" |
| í…Œë§ˆ | âŒ ì—†ìŒ | "ì¹´í˜ ì°½ê°€ íŒ¨í„´ 93% ì¼ì¹˜" |
| í”¼ë“œë°± | ì‹œê°ì ë§Œ | í–…í‹± + ìŒì„± + ì‹œê° |

---

## ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥

### ì˜¤í”„ë¼ì¸ í•™ìŠµ (RTX 4070 Super)
```
ì „ì²´ 2,132ì¥ ê¸°ì¤€:
- Grounding DINO ê°ì²´ ê²€ì¶œ: 1ì‹œê°„
- RTMPose Pose Type ë¶„ë¥˜: 30ë¶„
- Depth Anything V2 ê¹Šì´ ë¶„ì„: 40ë¶„
- í†µê³„ ê³„ì‚°: 10ë¶„

ì´: 2-3ì‹œê°„
ê²°ê³¼ë¬¼: 2-5MB JSON

MVP 300ì¥ ê¸°ì¤€:
- ì´ 30ë¶„
```

### ì‹¤ì‹œê°„ ì„±ëŠ¥ (iPhone 15 Pro)
```
- YOLO bbox: 10ms
- MiDaS Small: 50ms
- ê³„ì‚° + ë§¤ì¹­: 5ms
- í”¼ë“œë°±: 1ms

ì´: 66ms â†’ 15fps âœ…

ë°°í„°ë¦¬: ì¤‘ê°„ (MiDaS ì£¼ìš” ì†Œëª¨)
ë°œì—´: ë‚®ìŒ
```

---

## ğŸ¯ ì‹¤í–‰ ì „ëµ

### í˜„ì¬ ë°ì´í„° í™œìš© (2,132ì¥)

**Step 1: ìë™ ë¶„ë¥˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰**
```python
# auto_classify_all.py
for image in all_2132_images:
    theme = grounding_dino_classify_theme(image)
    pose_type = rtmpose_classify_pose(image)
    move_to(f"{theme}/{pose_type}/")
```

**Step 2: íŠ¹ì§• ì¶”ì¶œ**
```bash
python extract_features_full.py \
    --input_dir classified_full \
    --use_all_images True
```

**Step 3: íŒ¨í„´ ìƒì„±**
```bash
# ì¶œë ¥: patterns_full_v1.json (2-5MB)
```

### MVP í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ ê²€ì¦)

300ì¥ë§Œ ì„ ë³„í•˜ì—¬ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸:
```
cafe_indoor: 150ì¥
park_nature: 100ì¥
street_urban: 50ì¥

â†’ 30ë¶„ ë‚´ ì™„ë£Œ
â†’ ê¸°ëŠ¥ ê²€ì¦ í›„ ì „ì²´ í™•ì¥
```

---

## ğŸš€ ê°œë°œ ë¡œë“œë§µ

### Phase 0: ë°ì´í„° ì¤€ë¹„ (í˜„ì¬)
```
âœ… 2,132ì¥ ì´ë¯¸ì§€ í™•ë³´ (ì™„ë£Œ)
â¬œ ìë™ ë¶„ë¥˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
â¬œ Theme Ã— Pose Type í´ë” êµ¬ì„±
```

### Phase 1: íŠ¹ì§• ì¶”ì¶œ (1ì¼)
```
â¬œ RTX 4070S í™˜ê²½ ì„¸íŒ…
â¬œ Grounding DINO, RTMPose, Depth Anything V2 ì„¤ì¹˜
â¬œ extract_features_full.py ì‹¤í–‰
â¬œ patterns_full_v1.json ìƒì„±
```

### Phase 2: iOS ì•± í†µí•© (1ì£¼)
```
â¬œ JSON íŒŒì¼ ì•±ì— ë‚´ì¥
â¬œ íŒ¨í„´ ë§¤ì¹­ ë¡œì§ êµ¬í˜„
â¬œ ì‹¤ì‹œê°„ í”¼ë“œë°± UI
```

### Phase 3: í…ŒìŠ¤íŠ¸ ë° ê°œì„  (1ì£¼)
```
â¬œ ì‹¤ì œ ì´¬ì˜ í…ŒìŠ¤íŠ¸
â¬œ í”¼ë“œë°± ì •í™•ë„ ì¸¡ì •
â¬œ ì‚¬ìš©ì í…ŒìŠ¤íŠ¸
â¬œ íŒŒë¼ë¯¸í„° íŠœë‹
```

---

## ğŸ”‘ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### 1. Hierarchical Patternì˜ ì¤‘ìš”ì„±
```
âŒ ì˜ëª»ëœ ë°©ì‹:
   cafe_indoor ì „ì²´ í‰ê·  (ì•‰ì€ ë°˜ì‹  + ì„œìˆëŠ” ì „ì‹  ì„ì„)
   â†’ í†µê³„ê°€ ë¬´ì˜ë¯¸!

âœ… ì˜¬ë°”ë¥¸ ë°©ì‹:
   cafe_indoor_half_body (ì•‰ì€ ë°˜ì‹ ë§Œ)
   cafe_indoor_full_body (ì„œìˆëŠ” ì „ì‹ ë§Œ)
   â†’ ê°ê° ì˜ë¯¸ìˆëŠ” í†µê³„!

í•µì‹¬: Theme + Pose Type ì¡°í•©ì´ í•„ìˆ˜!
```

### 2. RTMPose vs YOLO
```
í˜„ì¬ ì•± êµ¬ì¡° (RTMPoseRunner.swift):
1. YOLOX â†’ Person bbox ê²€ì¶œ (ë¹ ë¦„)
2. RTMPose â†’ 133ê°œ keypoints (ì •ë°€)

ì™œ ë‘˜ ë‹¤ í•„ìš”?
- YOLOX: ë¹ ë¥¸ person detection
- RTMPose: ì–¼êµ´ ëœë“œë§ˆí¬ í¬í•¨ ì •ë°€ pose

v1.5ì—ì„œ:
- YOLOX bbox â†’ êµ¬ë„ ë¶„ì„ (ìœ„ì¹˜, í¬ê¸°, ì—¬ë°±)
- RTMPose keypoints â†’ Pose Type íŒì • (half/full/upper)
```

### 3. Occlusion Handling (ê°€ë¦¼ ëŒ€ì‘)
```
ë¬¸ì œ: ì˜·/í…Œì´ë¸”ë¡œ ê´€ì ˆ ê°€ë ¤ì§

í•´ê²° ì „ëµ:
1. Confidence threshold ë‚®ê²Œ (0.3)
2. ë³´ì´ëŠ” ê´€ì ˆë§Œìœ¼ë¡œ íŒë‹¨
3. Multi-frame averaging (ì‹¤ì‹œê°„)
4. Conservative classification
   ì˜ˆ: ë¶ˆí™•ì‹¤í•˜ë©´ "upper_body"

RTMPose ì¥ì :
- 133ê°œ keypoint â†’ ì–¼êµ´ ëœë“œë§ˆí¬ í™œìš© ê°€ëŠ¥
- Confidence per keypoint
- Robust to partial occlusion
```

### 4. Grounding DINOì˜ ì—­í• 
```
âœ… í•  ìˆ˜ ìˆëŠ” ê²ƒ:
   - ê°ê´€ì  ì¸¡ì • (ì–´ë””ì— ë­ê°€ ìˆëŠ”ì§€)
   - ì¸ë¬¼ + ë°°ê²½ ê°ì²´ ìœ„ì¹˜
   - í…Œë§ˆ ë¶„ë¥˜ (rules-based)

âŒ í•  ìˆ˜ ì—†ëŠ” ê²ƒ:
   - ì£¼ê´€ì  íŒë‹¨ (ì¢‹ì€ êµ¬ë„ì¸ì§€)
   - ì—¬ë°±ì´ ì ì ˆí•œì§€

â†’ ì˜¤í”„ë¼ì¸ í•™ìŠµì—ë§Œ ì‚¬ìš©!
```

### 5. ì—¬ë°± íŒë‹¨ì˜ ì›ë¦¬
```
Grounding DINO = ì¸¡ì • ë„êµ¬
í†µê³„ ë¶„ì„ = íŒ¨í„´ ë°œê²¬

"ì¢‹ì€ ì‚¬ì§„" 500ì¥ ì¸¡ì •
â†’ í‰ê·  ì—¬ë°± 22%-46%
â†’ ì´ê±¸ "ì´ìƒì  ì—¬ë°±"ìœ¼ë¡œ ì‚¬ìš©

â†’ ê·€ë‚©ì  ì¶”ë¡ !
```

### 6. ì••ì¶•ê° ì¸¡ì •
```
EXIF Focal Length: í¸ì§‘ëœ ì‚¬ì§„ì—” ì—†ìŒ âŒ
Depth Map: í•­ìƒ ê³„ì‚° ê°€ëŠ¥ âœ…

Compression Index:
- 0 = ê´‘ê° (í° depth ë²”ìœ„)
- 1 = ë§ì› (ì‘ì€ depth ë²”ìœ„)

â†’ "ê°€ê¹Œì´ vs ë©€ë¦¬+ì¤Œ" êµ¬ë¶„ ê°€ëŠ¥!
```

### 7. ì¹´ë©”ë¼ ê°ë„ ê°ì§€
```
âŒ Depth Map Gradient: ë¶€ì •í™•
âœ… iPhone Gyroscope: 95% ì •í™•ë„

CMMotionManager:
- pitch: ì¹´ë©”ë¼ ìƒí•˜ ê°ë„
- roll: ì¹´ë©”ë¼ ì¢Œìš° ê¸°ìš¸ê¸°
- ì‹¤ì‹œê°„, ë°°í„°ë¦¬ ì˜í–¥ ê±°ì˜ ì—†ìŒ

â†’ ë ˆí¼ëŸ°ìŠ¤ ì‚¬ì§„ì˜ angleê³¼ ë¹„êµ!
```

---

## ğŸ’¡ ì™œ ì´ ë°©ì‹ì´ ë˜‘ë˜‘í•œê°€?

### ì¥ì 

1. **Grounding DINO ê°•ì  í™œìš©**
   - ëŠë¦¬ì§€ë§Œ ì •ë°€ â†’ ì˜¤í”„ë¼ì¸ í•™ìŠµì—
   - ë°°ê²½ ê°ì²´ ì¸ì‹ â†’ í…Œë§ˆ ë¶„ë¥˜ì—

2. **ì‹¤ì‹œê°„ì€ ê²½ëŸ‰ ëª¨ë¸**
   - YOLO + MiDaS Small
   - ë¯¸ë¦¬ í•™ìŠµí•œ íŒ¨í„´ í™œìš©

3. **ì‚¬ìš©ìëŠ” ì°¨ì´ ëª» ëŠë‚Œ**
   - í”¼ë“œë°± ì¦‰ê°ì  (66ms)
   - ì •í™•ë„ 85-90%ë©´ ì¶©ë¶„
   - ë„¤íŠ¸ì›Œí¬ ë¶ˆí•„ìš”

4. **í™•ì¥ ê°€ëŠ¥**
   - ìƒˆ í…Œë§ˆ ì¶”ê°€ ì‰¬ì›€
   - ëª¨ë¸ ì—…ë°ì´íŠ¸ ë…ë¦½ì 
   - JSONë§Œ êµì²´

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

1. ìë™ ë¶„ë¥˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (auto_classify_all.py)
2. RTX 4070 Super í™˜ê²½ ì„¸íŒ…

---

## ğŸ†• v1.5.1 - ê°œì„ ëœ ì‹¤ì‹œê°„ í”¼ë“œë°± ì‹œìŠ¤í…œ

### 1. ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ê¸°ë°˜ í…Œë§ˆ ìë™ ê°ì§€

**ê¸°ì¡´ ë¬¸ì œ:**
```swift
// ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ í…Œë§ˆ ê°ì§€ ë¶ˆê°€ëŠ¥
// íœ´ë¦¬ìŠ¤í‹±: ë°ê¸°, ìƒ‰ì˜¨ë„ â†’ ë¶€ì •í™• (60-70%)
```

**ì™„ë²½í•œ í•´ê²°ì±…:**
```swift
class ReferenceBasedThemeDetector {
    func detectTheme(from referenceImage: ReferenceImage) -> Theme {
        // ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ëŠ” ì´ë¯¸ ë¶„ë¥˜ëœ í´ë”ì— ìˆìŒ
        // cafe_indoor/medium_shot/IMG_001.jpg
        let pathComponents = referenceImage.path.components
        return Theme(pathComponents[0])  // "cafe_indoor"
    }
}

// ì •í™•ë„: 100% (ì¶”ì¸¡ ë¶ˆí•„ìš”!)
```

**ì›Œí¬í”Œë¡œìš°:**
```
ì‚¬ìš©ì: ê°¤ëŸ¬ë¦¬ì—ì„œ ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ì„ íƒ
â†“
ì•±: ë©”íƒ€ë°ì´í„° ì½ê¸° (í´ë” êµ¬ì¡°ì—ì„œ)
â†“
theme = "cafe_indoor", pose = "medium_shot"
â†“
íŒ¨í„´ ë¡œë“œ: patterns["cafe_indoor_medium_shot_portrait"]
```

### 2. ê°œì„ ëœ ì¢…íš¡ë¹„ ì‹œìŠ¤í…œ (íšŒì „ í†µí•©)

**í•µì‹¬ í†µì°°: 9:16ê³¼ 16:9ëŠ” ê°™ì€ ë¹„ìœ¨, íšŒì „ë§Œ ë‹¤ë¦„**

```python
# ë¬¸ì œ: ë¶ˆí•„ìš”í•œ ì¤‘ë³µ
9:16 (portrait) vs 16:9 (landscape) â†’ ì‹¤ì œë¡œëŠ” ê°™ì€ ë¹„ìœ¨

# í•´ê²°: ì¢…íš¡ë¹„ì™€ ë°©í–¥ ë¶„ë¦¬
aspect_ratio = "16:9"  # ë¹„ìœ¨ (3ê°€ì§€: 1:1, 4:3, 16:9)
orientation = "portrait" or "landscape"  # ë°©í–¥

# ê²°ê³¼: 15ê°œ íŒ¨í„´ìœ¼ë¡œ ë‹¨ìˆœí™”
5 themes Ã— 4 poses Ã— 3 aspect_types = 60 â†’ ì‹¤ì œ 15ê°œ ìœ ë‹ˆí¬ íŒ¨í„´
```

**êµ¬í˜„:**
```python
def get_aspect_ratio_info(width, height) -> (aspect_ratio, orientation):
    # ë°©í–¥ íŒë‹¨
    if width > height:
        orientation = "landscape"
        ratio = width / height
    else:
        orientation = "portrait"
        ratio = height / width  # í•­ìƒ í°ìˆ˜/ì‘ì€ìˆ˜

    # ì¢…íš¡ë¹„ íŒë‹¨
    if abs(ratio - 1.0) < 0.1: return "1:1", orientation
    elif abs(ratio - 1.33) < 0.1: return "4:3", orientation
    elif abs(ratio - 1.78) < 0.1: return "16:9", orientation

ASPECT_PATTERNS = {
    "1:1": "square",     # ì •ë°©í˜•
    "4:3": "standard",   # ì „í†µì  ì‚¬ì§„
    "16:9": "wide"       # ì™€ì´ë“œìŠ¤í¬ë¦°
}
```

**ë°©í–¥ ì¡°ì • í•¨ìˆ˜ (íšŒì „ ëŒ€ì‘):**
```python
def apply_orientation_adjustment(margins, source_orient, target_orient):
    """
    íŒ¨í„´ì„ ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ ì ìš©í•  ë•Œ ì—¬ë°± ì¡°ì •
    ì˜ˆ: landscape íŒ¨í„´ â†’ portrait ì´¬ì˜
    """
    if source_orient == target_orient:
        return margins  # ê°™ì€ ë°©í–¥ì´ë©´ ê·¸ëŒ€ë¡œ

    if source_orient == "landscape" and target_orient == "portrait":
        # ê°€ë¡œâ†’ì„¸ë¡œ: ì¢Œìš° ì—¬ë°±ì„ ìƒí•˜ë¡œ, ìƒí•˜ë¥¼ ì¢Œìš°ë¡œ ìŠ¤ì™‘
        return {
            "left": margins["top"],
            "right": margins["bottom"],
            "top": margins["left"],
            "bottom": margins["right"]
        }

    # ì„¸ë¡œâ†’ê°€ë¡œë„ ë™ì¼í•˜ê²Œ ìŠ¤ì™‘
    # squareëŠ” íšŒì „ ë¬´ê´€í•˜ë¯€ë¡œ ì¡°ì • ë¶ˆí•„ìš”
```

### 3. ì ì‘í˜• íŒ¨í„´ í´ë°± ê³„ì¸µêµ¬ì¡°

**í•­ìƒ ê°€ì´ë“œ ì œê³µ ë³´ì¥:**
```python
def get_pattern_adaptive(theme, pose, aspect_ratio):
    """
    5ë‹¨ê³„ í´ë°±ìœ¼ë¡œ í•­ìƒ íŒ¨í„´ ì œê³µ
    """
    # 1ìˆœìœ„: ì •í™• ë§¤ì¹­ (ìµœê³  í’ˆì§ˆ)
    key = f"{theme}_{pose}_{get_cluster(aspect_ratio)}"
    if key in patterns:
        return patterns[key], "high"

    # 2ìˆœìœ„: ì¢…íš¡ë¹„ ë¬´ì‹œ (í…Œë§ˆ+í¬ì¦ˆ ìœ ì§€)
    key = f"{theme}_{pose}_all"
    if key in patterns:
        show_notice("ì¢…íš¡ë¹„ë³„ ë°ì´í„° ë¶€ì¡±, ì¼ë°˜ íŒ¨í„´ ì‚¬ìš©")
        return patterns[key], "medium"

    # 3ìˆœìœ„: í…Œë§ˆ ë¬´ì‹œ (í¬ì¦ˆ+í´ëŸ¬ìŠ¤í„° ìœ ì§€)
    key = f"all_{pose}_{get_cluster(aspect_ratio)}"
    if key in patterns:
        show_notice("í•´ë‹¹ í…Œë§ˆ ë°ì´í„° ë¶€ì¡±")
        return patterns[key], "low"

    # 4ìˆœìœ„: í¬ì¦ˆë§Œ (ë²”ìš©)
    key = f"all_{pose}_all"
    if key in patterns:
        return patterns[key], "generic"

    # 5ìˆœìœ„: ê¸°ë³¸ ê·œì¹™
    return get_default_rules(), "default"
```

### 4. ì‹¤ì‹œê°„ í”¼ë“œë°± ê°œì„ 

**ìƒíƒœ ê¸°ë°˜ ì—…ë°ì´íŠ¸:**
```swift
enum FeedbackState {
    case searching      // ë¹ ë¥¸ ì—…ë°ì´íŠ¸ (30 fps)
    case converging     // ì¤‘ê°„ ì—…ë°ì´íŠ¸ (10 fps)
    case locked         // ëŠë¦° ì—…ë°ì´íŠ¸ + í–…í‹±
}

func updateFeedback(frame: Frame, pattern: Pattern) {
    let deviation = calculateDeviation(frame, pattern)

    switch state {
    case .searching:
        if deviation < 0.15 {
            state = .converging
            updateFrequency = 10
        }

    case .converging:
        if deviation < 0.05 {
            state = .locked
            hapticSuccess()  // ì§„ë™ í”¼ë“œë°±
            showMessage("ì™„ë²½! ğŸ“¸")
        }

    case .locked:
        if deviation > 0.15 {
            state = .searching  // ë‹¤ì‹œ ì°¾ê¸°
        }
    }
}
```

**ë‹¤ì¤‘ ì§€í‘œ ì ìˆ˜:**
```swift
struct FramingScore {
    let marginScore: Float    // 40% ê°€ì¤‘ì¹˜
    let positionScore: Float  // 30%
    let sizeScore: Float      // 20%
    let aspectScore: Float    // 10%

    var overall: Float {
        marginScore * 0.4 + positionScore * 0.3 +
        sizeScore * 0.2 + aspectScore * 0.1
    }

    func feedback() -> String {
        if overall > 0.9 { return "ì™„ë²½! ğŸ“¸" }

        // ê°€ì¥ ì•½í•œ ë¶€ë¶„ ê°œì„  ê°€ì´ë“œ
        let weakest = findWeakest()
        switch weakest {
        case .margins: return "ì˜¤ë¥¸ìª½ ê³µê°„ ì¡°ê¸ˆ ë” â†’"
        case .position: return "ì‚´ì§ë§Œ ìœ„ë¡œ â†‘"
        case .size: return "ì¡°ê¸ˆë§Œ ë’¤ë¡œ ğŸ‘£"
        }
    }
}
```

### 5. ìˆ˜í•™ì  ì¢…íš¡ë¹„ ë³€í™˜ (Phase 2)

**ì •í™•í•œ íŒ¨í„´ ë³€í™˜ (95%+ ì •í™•ë„):**
```python
def adaptive_pattern_transform(pattern, source_aspect, target_aspect):
    """
    ì¢…íš¡ë¹„ ê°„ ìˆ˜í•™ì  ë³€í™˜
    ì˜ˆ: 4:3 ë ˆí¼ëŸ°ìŠ¤ â†’ 9:16 ì´¬ì˜
    """
    source_ratio = parse_ratio(source_aspect)  # 1.33
    target_ratio = parse_ratio(target_aspect)  # 0.56

    # ê·¹ë‹¨ì  ë³€í™˜ ê°ì§€
    if source_ratio > 1 and target_ratio < 1:
        # ê°€ë¡œí˜• â†’ ì„¸ë¡œí˜•
        transform_type = "landscape_to_portrait"

        # ì—¬ë°± ì¬ë¶„ë°° í•„ìš”
        margin_redistribution = True

        # ì¸ë¬¼ í¬ê¸° ì¡°ì • (ì„¸ë¡œí˜•ì€ ë” í¬ê²Œ)
        size_scale = 1.2

    # ì—¬ë°± ì¬ë¶„ë°°
    if margin_redistribution:
        # ê°€ë¡œ ì—¬ë°±ì˜ 30%ë¥¼ ì„¸ë¡œë¡œ ì´ë™
        total_h = pattern.margins.left + pattern.margins.right
        total_v = pattern.margins.top + pattern.margins.bottom

        transfer = total_h * 0.3
        new_h = total_h * 0.7
        new_v = total_v + transfer

        return {
            'margins': {
                'left': new_h * 0.4,
                'right': new_h * 0.6,
                'top': new_v * 0.4,
                'bottom': new_v * 0.6
            },
            'size': pattern.size * size_scale
        }
```

### 6. ì‹œê°ì  í”¼ë“œë°± UI

```swift
// ì‹¤ì‹œê°„ ì˜¤ë²„ë ˆì´
struct MarginGuides: View {
    var body: some View {
        ZStack {
            // ëª©í‘œ bbox (ì´ˆë¡ ì ì„ )
            Rectangle()
                .stroke(style: StrokeStyle(dash: [5, 5]))
                .foregroundColor(.green.opacity(0.6))
                .frame(target.bbox)

            // í˜„ì¬ bbox (ë…¸ë€ ì‹¤ì„ )
            Rectangle()
                .stroke(Color.yellow, lineWidth: 3)
                .frame(current.bbox)

            // ë°©í–¥ í™”ì‚´í‘œ
            if needsAdjustment {
                Arrow(direction: adjustmentDirection)
                    .foregroundColor(.white)
                    .font(.system(size: 60))
            }
        }
    }
}

// í•˜ë‹¨ HUD
struct MarginMeter: View {
    var body: some View {
        HStack {
            // ì—¬ë°± ë¯¸í„° (ìƒ‰ìƒìœ¼ë¡œ ìƒíƒœ í‘œì‹œ)
            ForEach(["L", "R", "T", "B"], id: \.self) { side in
                ProgressView(value: current[side], total: target[side])
                    .tint(marginColor(current[side], target[side]))
                Text("\(side): \(Int(current[side] * 100))%")
            }
        }
    }

    func marginColor(_ current: Float, _ target: Float) -> Color {
        let error = abs(current - target)
        if error < 0.05 { return .green }     // ì™„ë²½
        else if error < 0.15 { return .yellow } // ê·¼ì ‘
        else { return .red }                    // ì¡°ì • í•„ìš”
    }
}
```

### 7. ì„±ëŠ¥ ì˜ˆì‚°

| Component | Latency | Failure Mode |
|-----------|---------|--------------|
| Person Detection | < 16ms | Temporal smoothing |
| Theme Detection | < 0.1ms | Fallback patterns |
| Pattern Lookup | < 0.1ms | Generic rules |
| Scoring | < 1ms | N/A |
| UI Rendering | < 16ms | Reduce frequency |
| **Total** | **< 35ms** | Maintains 30 fps |

---

## ğŸ“Š ì˜ˆìƒ íš¨ê³¼

### ê°œì„  ì „ vs ê°œì„  í›„

| ë¬¸ì œ | v1.5 (ê¸°ì¡´) | v1.5.1 (ê°œì„ ) | íš¨ê³¼ |
|-----|------------|--------------|------|
| í…Œë§ˆ ê°ì§€ | íœ´ë¦¬ìŠ¤í‹± 60-70% | ë ˆí¼ëŸ°ìŠ¤ ê¸°ë°˜ 100% | +40% ì •í™•ë„ |
| ìƒ˜í”Œ ë¶€ì¡± | 100ê°œ ì¡°í•©, ëŒ€ë¶€ë¶„ < 20ìƒ˜í”Œ | 60ê°œ ì¡°í•© + í´ë°± | ëª¨ë“  ê²½ìš° íŒ¨í„´ ì œê³µ |
| ì¢…íš¡ë¹„ ì™œê³¡ | ë¬´ì‹œí•˜ê±°ë‚˜ ë¶„ë¦¬ | í´ëŸ¬ìŠ¤í„° + ë³´ì • | 80% ì¼€ì´ìŠ¤ ì»¤ë²„ |
| í”¼ë“œë°± ë–¨ë¦¼ | ë§¤ í”„ë ˆì„ ì—…ë°ì´íŠ¸ | ìƒíƒœ ê¸°ë°˜ | ì•ˆì •ì  UX |
| ì‚¬ìš©ì í˜¼ë€ | ì—¬ëŸ¬ ì§€í‘œ ê°œë³„ í‘œì‹œ | í†µí•© ì ìˆ˜ + ìš°ì„ ìˆœìœ„ | ëª…í™•í•œ ê°€ì´ë“œ |

### êµ¬í˜„ ë¡œë“œë§µ

**Phase 1 (1ì£¼):**
- âœ… GroundingDINO API ì—ëŸ¬ ìˆ˜ì •
- â¬œ ì¢…íš¡ë¹„ í´ëŸ¬ìŠ¤í„° ë¡œì§ ì¶”ê°€
- â¬œ ê°„ë‹¨ ë³´ì • ê³„ìˆ˜ ì ìš©
- â¬œ ì ì‘í˜• íŒ¨í„´ ì§‘ê³„ êµ¬í˜„
- â¬œ íŠ¹ì§• ì¶”ì¶œ ì¬ì‹¤í–‰

**Phase 2 (1ì£¼):**
- â¬œ iOS ì•± ë ˆí¼ëŸ°ìŠ¤ ê¸°ë°˜ í…Œë§ˆ ê°ì§€
- â¬œ ìƒíƒœ ê¸°ë°˜ í”¼ë“œë°± êµ¬í˜„
- â¬œ ì‹œê°ì  UI ê°œì„ 
- â¬œ í–…í‹± í”¼ë“œë°± ì¶”ê°€

**Phase 3 (ë‚˜ì¤‘ì—):**
- â¬œ ìˆ˜í•™ì  ì¢…íš¡ë¹„ ë³€í™˜
- â¬œ A/B í…ŒìŠ¤íŠ¸
- â¬œ ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜
3. Grounding DINO + Depth Anything V2 ì„¤ì¹˜
4. 2,132ì¥ íŠ¹ì§• ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
5. JSON DB ìƒì„±
6. iOS ì•± í†µí•©

---

**ìµœì´ˆ ì‘ì„±ì¼:** 2025-11-30
**ìµœì¢… ì—…ë°ì´íŠ¸:** 2025-12-01
**ë¸Œëœì¹˜:** v1.5_learning
**ë²„ì „:** 1.5

## ğŸ“ ë³€ê²½ ì´ë ¥

### v1.5 (2025-12-01) - ì‹¤ì‹œê°„ í”¼ë“œë°± ì‹œìŠ¤í…œ ëŒ€í­ ê°œì„ 
- âœ… **ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ê¸°ë°˜ í…Œë§ˆ ìë™ ê°ì§€**
  - í…Œë§ˆ ê°ì§€ ë¬¸ì œ ì™„ë²½ í•´ê²° (100% ì •í™•ë„)
  - ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° í™œìš©
  - On-device íœ´ë¦¬ìŠ¤í‹± ì œê±°
- âœ… **ì¢…íš¡ë¹„ ì‹œìŠ¤í…œ ê°œì„  (íšŒì „ í†µí•©)**
  - 9:16ê³¼ 16:9ë¥¼ ê°™ì€ ë¹„ìœ¨ë¡œ ì¸ì‹
  - ì¢…íš¡ë¹„(3ê°€ì§€)ì™€ ë°©í–¥(2ê°€ì§€) ë¶„ë¦¬
  - 100â†’15ê°œ ìœ ë‹ˆí¬ íŒ¨í„´ìœ¼ë¡œ ë‹¨ìˆœí™”
  - ë°©í–¥ ê°„ ì—¬ë°± ë³€í™˜ í•¨ìˆ˜ ì¶”ê°€
- âœ… **RTMPose ì˜ì¡´ì„± ì œê±°**
  - í´ë” êµ¬ì¡°ì—ì„œ pose_type ì§ì ‘ ì‚¬ìš©
  - GroundingDINOë¡œ person bbox íšë“
  - ì˜ì¡´ì„± ë¬¸ì œ í•´ê²° ë° íŒŒì´í”„ë¼ì¸ ë‹¨ìˆœí™”
- âœ… **ì ì‘í˜• íŒ¨í„´ í´ë°± ê³„ì¸µêµ¬ì¡°**
  - 1ìˆœìœ„: theme Ã— pose Ã— cluster
  - 2ìˆœìœ„: theme Ã— pose (ì¢…íš¡ë¹„ ë¬´ì‹œ)
  - 3ìˆœìœ„: pose Ã— cluster
  - 4ìˆœìœ„: pose only
  - 5ìˆœìœ„: ê¸°ë³¸ ê·œì¹™
  - í•­ìƒ ê°€ì´ë“œ ì œê³µ ë³´ì¥
- âœ… **ì‹¤ì‹œê°„ í”¼ë“œë°± ê°œì„ **
  - ìƒíƒœ ê¸°ë°˜ ì—…ë°ì´íŠ¸ (searching/converging/locked)
  - ë‹¤ì¤‘ ì§€í‘œ ì ìˆ˜ ì‹œìŠ¤í…œ
  - ì‹œê°ì  í”¼ë“œë°± UI ëª…ì„¸
  - Haptic feedback ì¶”ê°€
- âœ… **ìˆ˜í•™ì  ì¢…íš¡ë¹„ ë³€í™˜ (Phase 2)**
  - ì¢…íš¡ë¹„ ê°„ ì •í™•í•œ íŒ¨í„´ ë³€í™˜
  - ì—¬ë°± ì¬ë¶„ë°° ì•Œê³ ë¦¬ì¦˜
  - 95%+ ì •í™•ë„ ëª©í‘œ

### v1.4 (2025-12-01) - íŠ¹ì§• ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ì™„ì „ ì„¤ê³„
- âœ… **Step 5: Aspect Ratio ì¸ì‹ ë° ì²˜ë¦¬ ì¶”ê°€**
  - ì¢…íš¡ë¹„(1:1, 4:3, 9:16, 16:9) ìë™ ê°ì§€
  - ì¢…íš¡ë¹„ë³„ ìµœì  ì—¬ë°± ì°¨ë³„í™”
  - íŒ¨í„´ ì„¸ë¶„í™”: theme Ã— pose_type Ã— aspect_ratio
  - ì‹¤ì‹œê°„ ì¢…íš¡ë¹„ ê°ì§€ ë° fallback ì „ëµ
- âœ… **Step 5-1: Memory-Optimized Feature Extraction Pipeline**
  - Batch Processing (ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬)
  - Checkpoint/Resume System (í¬ë˜ì‹œ ë³µêµ¬)
  - Incremental Statistics (Welford's Algorithm)
  - GPU Memory Management (ë©”ëª¨ë¦¬ ìµœì í™”)
  - Progress Tracking (tqdm ì§„í–‰ í‘œì‹œ)
  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 4GB ì´í•˜ ìœ ì§€
  - ì •ë³´ ì†ì‹¤ 0% ë³´ì¥
- âœ… **Step 5-2: Three-Tier Confidence System**
  - Tier 1: Certain Features (95%+ ì‹ ë¢°ë„) - ìœ„ì¹˜, í¬ê¸°, ì—¬ë°±
  - Tier 2: Useful Features (70-90% ì‹ ë¢°ë„) - ì••ì¶•ê°, ë°°ê²½ ê°ì²´
  - Tier 3: Experimental Features (50-70% ì‹ ë¢°ë„) - ê°ë„ ì¶”ì •
  - ì‹ ë¢°ë„ ê¸°ë°˜ í”¼ë“œë°± ìš°ì„ ìˆœìœ„ ì„¤ì •
  - íŒ¨í„´ JSONì— ì‹ ë¢°ë„ ë©”íƒ€ë°ì´í„° í¬í•¨
- âœ… **Step 5-3: ë‘ ê°€ì§€ ì¶œë ¥ í˜•ì‹**
  - patterns_full_v1.json (2-5MB) - í•™ìŠµ/ë¶„ì„ìš©
  - patterns_app_v1.json (~50KB) - iOS ì•± ë‚´ì¥ìš©
  - ì•± í¬ê¸° ì˜í–¥ ìµœì†Œí™” (50KB ì¶”ê°€)
- âœ… **Phase 2.5: iOS ì•± í†µí•© ì „ëµ**
  - ê¸°ì¡´ rule-based ì‹œìŠ¤í…œ ë¶„ì„ (main ë¸Œëœì¹˜)
  - Hybrid Pattern-Based Approach ì„¤ê³„
  - ê¸°ì¡´ RTMPose ì½”ë“œ ì¬í™œìš© ì „ëµ
  - PatternBasedAnalyzer êµ¬í˜„ ê³„íš
  - Pose Type ìë™ íŒì • ë¡œì§
  - íŒ¨í„´ ê¸°ë°˜ í”¼ë“œë°± ìƒì„± ì•Œê³ ë¦¬ì¦˜
  - Fallback ì „ëµ (íŒ¨í„´ ì—†ì„ ë•Œ rule-based)
  - ì„±ëŠ¥ ìµœì í™” (Frame skip, ìºì‹±)
  - 4ì£¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš
  - A/B í…ŒìŠ¤íŠ¸ ì§€ì›

### v1.3 (2025-11-30)
- âœ… **ìƒˆë¡œìš´ 4-Type Shot ë¶„ë¥˜ ì²´ê³„ ë„ì…** (Samsung Display ê¸°ì¤€ ì°¸ê³ )
  - ê¸°ì¡´: closeup, upper_body, half_body, full_body
  - ì‹ ê·œ: **closeup, medium_shot, knee_shot, full_shot**
  - medium_shot = bust_shot + waist_shot í†µí•© (ì‹¤ìš©ì  ë¶„ë¥˜)
- âœ… **ì•‰ì€ ìì„¸ ê°ì§€ ë¡œì§ ì¶”ê°€**
  - ê³¨ë°˜-ë¬´ë¦ ê±°ë¦¬ / ìƒì²´ ê¸¸ì´ ë¹„ìœ¨ë¡œ íŒë‹¨
  - ratio < 0.6 â†’ ì•‰ì€ ìì„¸
  - ì•‰ì€ ìì„¸ì¼ ë•Œ Shot Type ë³´ì • ì ìš©
- âœ… **rtmlib (ONNX) ê¸°ë°˜ RTMPose êµ¬í˜„**
  - Mac M3ì—ì„œë„ ë™ì‘ ê°€ëŠ¥
  - ì†ë„: ~2 it/s (2,133ì¥ ì•½ 15ë¶„)
- âœ… ë¶„ë¥˜ ê¸°ì¤€ ëª…í™•í™” (í”„ë ˆì„ ë‚´ ìœ„ì¹˜ ê¸°ë°˜)

### v1.2 (2025-12-01)
- âœ… 2,132ì¥ ì „ì²´ í™œìš© ê³„íš ì¶”ê°€ (MVP ì œí•œ í•´ì œ)
- âœ… ì™„ì „ ìë™í™” One-Pass Processing ì¶”ê°€
- âœ… ë‹¤ì¤‘ ì‹ í˜¸ ê¸°ë°˜ Robust Classification
- âœ… ì‹¤ì§ˆì  ì´ì  ì„¹ì…˜ ì‹ ê·œ ì¶”ê°€
- âœ… ê°œì„ ëœ íŒŒì´í”„ë¼ì¸ ì„¹ì…˜ ì¶”ê°€
- âœ… ì‹¤í–‰ ì „ëµ êµ¬ì²´í™”

### v1.1 (2025-11-30)
- âœ… CLIP â†’ Grounding DINO rules-based + DINOv2 clusteringìœ¼ë¡œ ë³€ê²½
- âœ… Hierarchical Pattern êµ¬ì¡° ì¶”ê°€ (theme + pose_type)
- âœ… RTMPose 133 keypoints ê¸°ë°˜ Pose Type ìë™ ê²€ì¶œ ì¶”ê°€
- âœ… Occlusion Handling ì „ëµ ì¶”ê°€
- âœ… Gyroscope ê°ë„ ê°ì§€ (depth gradient ì œê±°)
- âœ… MVP Strategy ì¶”ê°€ (300ì¥ ìƒ˜í”Œ)
- âœ… ê°œë°œ ë¡œë“œë§µ MVP ê¸°ë°˜ìœ¼ë¡œ ì¬êµ¬ì„±
- âœ… RTMPose vs YOLO ëª…í™•í™”

### v1.0 (2025-11-30)
- ìµœì´ˆ ë¬¸ì„œ ì‘ì„±